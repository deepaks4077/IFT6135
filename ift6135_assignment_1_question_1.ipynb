{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of question number 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return (x > 0) * x\n",
    "\n",
    "def softmax(x, axis=1):\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=axis, keepdims=True)\n",
    "\n",
    "def onehot(y, n_classes):\n",
    "    o = np.zeros(shape=(y.shape[0], n_classes))\n",
    "    for i in range(y.shape[0]):\n",
    "        o[i, int(y[i])] = 1\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\n",
    "        self.W1_range = np.sqrt(6. / (input_size + hidden1_size))\n",
    "        self.W2_range = np.sqrt(6. / (hidden2_size + hidden1_size))\n",
    "        self.W3_range = np.sqrt(6. / (output_size + hidden2_size))\n",
    "        \n",
    "        self.W1 = np.random.uniform(-self.W1_range, self.W1_range, size=(hidden1_size, input_size))\n",
    "        #self.W1 = np.random.uniform()\n",
    "        #self.W1 = np.random.randn()\n",
    "        self.b1 = np.zeros(hidden1_size)\n",
    "        \n",
    "        self.W2 = np.random.uniform(-self.W2_range, self.W2_range, size=(hidden2_size, hidden1_size))\n",
    "        self.b2 = np.zeros(hidden2_size)\n",
    "        \n",
    "        self.W3 = np.random.uniform(-self.W3_range, self.W3_range, size=(output_size, hidden2_size))\n",
    "        self.b3 = np.zeros(output_size)\n",
    "        \n",
    "        print(\"W1 shape \", self.W1.shape)\n",
    "        print(\"b1 shape \", self.b1.shape)\n",
    "        print(\"W2 shape \", self.W2.shape)\n",
    "        print(\"b2 shape \", self.b2.shape)\n",
    "        print(\"W3 shape \", self.W3.shape)\n",
    "        print(\"b3 shape \", self.b3.shape)\n",
    "        \n",
    "        self.parameters = [self.b1, self.W1, self.b2, self.W2, self.b3, self.W3]\n",
    "        \n",
    "    def loop_fprop(self, x):\n",
    "        ha_1 = np.dot(self.W1, x) + self.b1\n",
    "        hs_1 = relu(ha_1)\n",
    "        \n",
    "        ha_2 = np.dot(self.W2, hs_1) + self.b2\n",
    "        hs_2 = relu(ha_2)\n",
    "        \n",
    "        oa = np.dot(self.W3, hs_2) + self.b3\n",
    "        os = softmax(oa, axis=0)\n",
    "        return ha_1, hs_1, ha_2, hs_2, oa, os\n",
    "        \n",
    "    def loop_bprop(self, y, x, ha_1, hs_1, ha_2, hs_2, oa, os, weight_decay=0):\n",
    "        dl_doa = os - y\n",
    "        \n",
    "        dl_dW3 = np.outer(dl_doa, hs_2) + weight_decay * self.W3\n",
    "        dl_db3 = dl_doa\n",
    "        \n",
    "        dl_dhs_2 = np.dot(self.W3.T, dl_doa)\n",
    "        dl_dha_2 = (ha_2 > 0) * dl_dhs_2\n",
    "        \n",
    "        dl_dW2 = np.outer(dl_dha_2, hs_1) + weight_decay * self.W2\n",
    "        dl_db2 = dl_dha_2\n",
    "        \n",
    "        dl_dhs_1 = np.dot(self.W2.T, dl_dha_2)\n",
    "        dl_dha_1 = (ha_1 > 0) * dl_dhs_1\n",
    "        \n",
    "        \n",
    "        dl_dW1 = np.outer(dl_dha_1, x) + weight_decay * self.W1\n",
    "        dl_db1 = dl_dha_1\n",
    "        \n",
    "        return dl_db1, dl_dW1, dl_db2, dl_dW2, dl_db3, dl_dW3\n",
    "    \n",
    "    def loop_loss(self, os, y):\n",
    "        return (y * (-np.log(os))).sum()\n",
    "    \n",
    "    def loop_finite_diff(self, x, y, eps=1e-5):\n",
    "        ha_1, hs_1, ha_2, hs_2, oa, os = self.loop_fprop(x)\n",
    "        gradients_bprop = self.loop_bprop(y, x, ha_1, hs_1, ha_2, hs_2, oa, os)\n",
    "        loss = self.loop_loss(os, y)\n",
    "        gradients_finite_diff = []\n",
    "        for p in self.parameters:\n",
    "            grad_fdiff = np.zeros(shape=p.shape)\n",
    "            for index, v in np.ndenumerate(p):\n",
    "                p[index] += eps\n",
    "                _, _, _, _, _, os = self.loop_fprop(x)\n",
    "                loss_diff = self.loop_loss(os, y)\n",
    "                grad_fdiff[index] = (loss_diff - loss) / eps\n",
    "                p[index] -= eps\n",
    "            gradients_finite_diff.append(grad_fdiff)\n",
    "        return gradients_finite_diff\n",
    "    \n",
    "#     def loop_train(self, data, target, mb_size=100, learning_rate=1e-1, weight_decay=0.):\n",
    "#         for i in range(data.shape[0] // mb_size):\n",
    "#             xi = data[i*mb_size:(i+1)*mb_size]\n",
    "#             yi = target[i*mb_size:(i+1)*mb_size]\n",
    "#             ha, hs, oa, os = self.mat_fprop(xi)\n",
    "#             average_loss, average_grads = self.loop_grad(xi, yi, weight_decay)\n",
    "#             for p, grad in zip(self.parameters, average_grads):\n",
    "#                 p -= learning_rate * grad\n",
    "#         return average_loss\n",
    "    \n",
    "    def loop_grad(self, x, y, weight_decay=0):\n",
    "        sum_grads = [np.zeros(shape=p.shape) for p in self.parameters]\n",
    "        sum_loss = 0\n",
    "        for xi, yi in zip(x, y):\n",
    "            ha_1, hs_1, ha_2, hs_2, oa, os = self.loop_fprop(xi)\n",
    "            grad = self.loop_bprop(yi, xi, ha_1, hs_1, ha_2, hs_2, oa, os, weight_decay)\n",
    "            sum_grads = [g1 + g2 for g1, g2 in zip(grad, sum_grads)]\n",
    "            sum_loss += self.loop_loss(os, yi)\n",
    "        average_grads = [g / x.shape[0] for g in sum_grads]\n",
    "        return sum_loss / x.shape[0], average_grads\n",
    "            \n",
    "#     def loop_predict(self, x):\n",
    "#         predictions = np.zeros(x.shape[0])\n",
    "#         for i in range(x.shape[0]):\n",
    "#             xi = x[i]\n",
    "#             _, _, _, os = self.loop_fprop(xi)\n",
    "#             predictions[i] = os.argmax()\n",
    "#         return predictions\n",
    "    \n",
    "    def mat_fprop(self, x):\n",
    "        #print(\"inside mat fprop\\n\")\n",
    "        #print(\"x shape \", x.shape)\n",
    "        #print(\"W1 shape \", self.W1.shape)\n",
    "        ha_1 = np.dot(x, self.W1.T) + self.b1\n",
    "        hs_1 = relu(ha_1)\n",
    "        \n",
    "        ha_2 = np.dot(hs_1, self.W2.T) + self.b2\n",
    "        hs_2 = relu(ha_2)\n",
    "        \n",
    "        oa = np.dot(hs_2, self.W3.T) + self.b3\n",
    "        os = softmax(oa, axis=1)\n",
    "        return ha_1, hs_1, ha_2, hs_2, oa, os\n",
    "    \n",
    "    def mat_bprop(self, y, x, ha_1, hs_1, ha_2, hs_2, oa, os, weight_decay=0):\n",
    "        bs = x.shape[0]\n",
    "        dl_doa = os - y\n",
    "        \n",
    "        dl_dW3 = np.dot(dl_doa.T, hs_2) / bs + weight_decay * self.W3\n",
    "        dl_db3 = dl_doa.mean(axis=0)\n",
    "        \n",
    "        dl_dhs_2 = np.dot(dl_doa, self.W3)\n",
    "        dl_dha_2 = (ha_2 > 0) * dl_dhs_2\n",
    "        \n",
    "        dl_dW2 = np.dot(dl_dha_2.T, hs_1) / bs + weight_decay * self.W2\n",
    "        dl_db2 = dl_dha_2.mean(axis=0)\n",
    "        \n",
    "        d1_dhs_1 = np.dot(dl_dha_2, self.W2)\n",
    "        dl_dha_1 = (ha_1 > 0) * d1_dhs_1\n",
    "        \n",
    "        dl_dW1 = np.dot(dl_dha_1.T, x) / bs + weight_decay * self.W1\n",
    "        dl_db1 = dl_dha_1.mean(axis=0)\n",
    "        \n",
    "        return dl_db1, dl_dW1, dl_db2, dl_dW2, dl_db3, dl_dW3\n",
    "    \n",
    "    def mat_loss(self, os, y):\n",
    "        return (y * (-np.log(os))).sum(axis=1).mean(axis=0)            \n",
    "        \n",
    "    def mat_train(self, data, target, mb_size=100, learning_rate=1e-2, weight_decay=0.):\n",
    "        for i in range(data.shape[0] // mb_size):\n",
    "            xi = data[i*mb_size:(i+1)*mb_size]\n",
    "            yi = target[i*mb_size:(i+1)*mb_size]\n",
    "            ha_1, hs_1, ha_2, hs_2, oa, os = self.mat_fprop(xi)\n",
    "            average_grads = self.mat_bprop(yi, xi, ha_1, hs_1, ha_2, hs_2, oa, os, weight_decay)\n",
    "            average_loss = self.mat_loss(os, yi)\n",
    "            for p, grad in zip(self.parameters, average_grads):\n",
    "                p -= learning_rate * grad\n",
    "        return average_loss\n",
    "    \n",
    "    \n",
    "    def mat_test(self, x, y):\n",
    "        _, _, _, _, _, os = self.mat_fprop(x)\n",
    "        return self.mat_loss(os, y), os.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check that the gradients are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the gradient using random sample\n",
    "mlp = MLP(2, 2, 2, 3)\n",
    "x = np.random.uniform(-1, 1, size=(2, ))\n",
    "y = np.zeros(shape=(3, ))\n",
    "y[1] = 1\n",
    "ha_1, hs_1, ha_2, hs_2, oa, os = mlp.loop_fprop(x)\n",
    "print('gradients computed by bprop ', mlp.loop_bprop(y, x, ha_1, hs_1, ha_2, hs_2, oa, os))\n",
    "print('gradients finite differences', mlp.loop_finite_diff(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Check the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the gradient using random sample\n",
    "mlp = MLP(2, 2, 2, 3)\n",
    "x = np.random.uniform(-1, 1, size=(3, 2))\n",
    "y = np.zeros(shape=(3, 3))\n",
    "y[0, 1] = 1\n",
    "y[1, 0] = 1\n",
    "y[2, 2] = 1\n",
    "print('gradients computed by bprop ', mlp.loop_grad(x, y)[1])\n",
    "\n",
    "grad_finitediff = [np.zeros(shape=p.shape) for p in mlp.parameters]\n",
    "for i in range(3):\n",
    "    grad_finitediff = [g + g2 / 3 for g, g2 in zip(grad_finitediff, mlp.loop_finite_diff(x[i, :], y[i, :]))]\n",
    "\n",
    "print('gradients finite differences', grad_finitediff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the gradient using random sample\n",
    "mlp = MLP(2, 2, 2, 3)\n",
    "x = np.random.uniform(-1, 1, size=(3, 2))\n",
    "y = np.zeros(shape=(3, 3))\n",
    "y[0, 1] = 1\n",
    "y[1, 0] = 1\n",
    "y[2, 2] = 1\n",
    "print('gradients computed by loop bprop ', mlp.loop_grad(x, y)[1])\n",
    "\n",
    "ha_1, hs_1, ha_2, hs_2, oa, os = mlp.mat_fprop(x)\n",
    "average_grads = mlp.mat_bprop(y, x, ha_1, hs_1, ha_2, hs_2, oa, os)\n",
    "print('gradients computed by mat bprop ', average_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either put the fashionmnist folder in the current folder or uncomment the next line that will download it\n",
    "!git clone https://github.com/zalandoresearch/fashion-mnist fashionmnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fashionmnist.utils import mnist_reader\n",
    "from random import shuffle\n",
    "\n",
    "X_train, y_train = mnist_reader.load_mnist('fashionmnist/data/fashion', kind='train')\n",
    "X_test, y_test = mnist_reader.load_mnist('fashionmnist/data/fashion', kind='t10k')\n",
    "\n",
    "indices = list(range(len(X_train)))\n",
    "shuffle(indices)\n",
    "\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "X_valid, y_valid = X_train[indices[55000:]], y_train[indices[55000:]]\n",
    "X_train, y_train = X_train[indices[:55000]], y_train[indices[:55000]]\n",
    "\n",
    "y_train_onehot = onehot(y_train, 10)\n",
    "y_valid_onehot = onehot(y_valid, 10)\n",
    "y_test_onehot = onehot(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "mnist_data = np.load('datasets/mnist.npy', encoding='latin1')\n",
    "X_train, y_train = mnist_data[0]\n",
    "X_valid, y_valid = mnist_data[1]\n",
    "X_test, y_test = mnist_data[2]\n",
    "\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_valid = X_valid.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "\n",
    "print('len X train ', len(X_train))\n",
    "print('len y train ', len(y_train))\n",
    "print('len X valid ', len(X_valid))\n",
    "print('len y valid ', len(y_valid))\n",
    "print('len X test ', len(X_test))\n",
    "print('len y test ', len(y_test))\n",
    "\n",
    "indices = list(range(len(X_train)))\n",
    "shuffle(indices)\n",
    "\n",
    "X_train = X_train / 255\n",
    "X_valid = X_valid / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "y_train_onehot = onehot(y_train, 10)\n",
    "y_valid_onehot = onehot(y_valid, 10)\n",
    "y_test_onehot = onehot(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])\n",
    "print(y_train_onehot[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timings matrix vs loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "mlp = MLP(784, 512, 512, 10)\n",
    "# start_time = time.time()\n",
    "# mlp.loop_train(X_train, y_train_onehot, mb_size=100)\n",
    "# time_loop = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "mlp.mat_train(X_train, y_train_onehot, mb_size=100)\n",
    "time_mat = time.time() - start_time\n",
    "\n",
    "#print('Time with loop implementation: %f seconds\\n' % time_loop +\n",
    "# 'Time with mat implementation: %f seconds' % time_mat)\n",
    "print('Time with mat implementation: %f seconds '%time_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute train/valid/test loss and accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(784, 512, 512, 10)\n",
    "\n",
    "train_accuracies, train_losses = [], []\n",
    "valid_accuracies, valid_losses = [], []\n",
    "test_accuracies, test_losses = [], []\n",
    "\n",
    "for e in range(20):\n",
    "    loss = mlp.mat_train(X_train, y_train_onehot, mb_size=100)\n",
    "    \n",
    "    loss_train, pred_train = mlp.mat_test(X_train, y_train_onehot)\n",
    "    loss_valid, pred_valid = mlp.mat_test(X_valid, y_valid_onehot)\n",
    "    loss_test, pred_test = mlp.mat_test(X_test, y_test_onehot)\n",
    "    valid_losses.append(loss_valid)\n",
    "    test_losses.append(loss_test)\n",
    "    #print(\"pred valid 0 \", pred_valid[1])\n",
    "    #print(\"y valid 0 \", y_valid[1])\n",
    "    valid_accuracies.append((pred_valid == y_valid).mean())\n",
    "    test_accuracies.append((pred_test == y_test).mean())\n",
    "    train_losses.append(loss_train)\n",
    "    train_accuracies.append((pred_train == y_train).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display training curves\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "axis = plt.subplot(1, 2, 1)\n",
    "axis.plot(range(1, len(train_losses)+1), train_losses, label='train')\n",
    "axis.plot(range(1, len(valid_losses)+1), valid_losses, label='valid')\n",
    "axis.plot(range(1, len(test_losses)+1), test_losses, label='test')\n",
    "axis.legend()\n",
    "axis.set_ylabel('Loss')\n",
    "axis.set_xlabel('Epochs')\n",
    "axis = plt.subplot(1, 2, 2)\n",
    "axis.plot(range(1, len(train_accuracies)+1), train_accuracies, label='train')\n",
    "axis.plot(range(1, len(valid_accuracies)+1), valid_accuracies, label='valid')\n",
    "axis.plot(range(1, len(test_accuracies)+1), test_accuracies, label='test')\n",
    "axis.legend()\n",
    "axis.set_ylabel('Accuracy')\n",
    "axis.set_xlabel('Epochs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
