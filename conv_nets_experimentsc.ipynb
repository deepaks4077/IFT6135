{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from math import ceil\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = './Assignments/Assignment 1/ift6135h19'\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    # Extends torchvision.datasets.ImageFolder\n",
    "\n",
    "    # override __getitem__ \n",
    "    def __getitem__(self, index):\n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        path = self.imgs[index][0]\n",
    "        tuple_and_path = (original_tuple + (path,))\n",
    "        return tuple_and_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# data_loader\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(5),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# print('Device: {}'.format(device))\n",
    "\n",
    "\n",
    "def data_loader(train_to_valid_ratio=0.8, \n",
    "                root_dir=ROOT_DIR, \n",
    "                batch_size=BATCH_SIZE):\n",
    "\n",
    "    train_valid_data = ImageFolderWithPaths(\n",
    "        os.path.join(root_dir, 'trainset'), data_transforms['train'])\n",
    "    \n",
    "    test_data = ImageFolderWithPaths(os.path.join(\n",
    "        root_dir, 'testset'), data_transforms['test'])\n",
    "    \n",
    "    print('class_to_idx: {}'.format(train_valid_data.class_to_idx))\n",
    "    class_to_idx = train_valid_data.class_to_idx\n",
    "    idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "    \n",
    "    print('Size of train_valid_data[0]: {}\\n'.format(len(train_valid_data[0][0].shape)))\n",
    "    \n",
    "    train_valid_data_size = len(train_valid_data)\n",
    "    train_valid_indices = list(range(train_valid_data_size))\n",
    "    split = int(np.ceil(train_to_valid_ratio * train_valid_data_size))\n",
    "#     print('split = {}'.format(split))\n",
    "    \n",
    "#     print('Image size = {}, Label = {}\\n'.format(train_valid_data[0][0].shape, train_valid_data[0][1]))\n",
    "\n",
    "    # shuffle the indices\n",
    "    np.random.shuffle(train_valid_indices)\n",
    "    train_indices, valid_indices = train_valid_indices[:split], train_valid_indices[split:]\n",
    "    \n",
    "    print('len(train_indices): {}'.format(len(train_indices)))\n",
    "    print('len(valid_indices): {}'.format(len(valid_indices)))\n",
    "    \n",
    "    print('Size of test data: {}'.format(len(test_data)))\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(valid_indices)\n",
    "\n",
    "    train_loader = DataLoader(train_valid_data, batch_size=batch_size, sampler=train_sampler)\n",
    "    valid_loader = DataLoader(train_valid_data, batch_size=batch_size, sampler=valid_sampler)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return (train_loader, valid_loader, test_loader, \n",
    "            class_to_idx, idx_to_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_to_idx: {'Cat': 0, 'Dog': 1}\n",
      "Size of train_valid_data[0]: 3\n",
      "\n",
      "split = 15999\n",
      "Image size = torch.Size([3, 64, 64]), Label = 0\n",
      "\n",
      "len(train_indices): 15999\n",
      "len(valid_indices): 3999\n",
      "Size of test data: 4999\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader, test_loader, class_to_idx, idx_to_class = data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGNet inspired model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.convolutional_layers = nn.Sequential(\n",
    "            # 1\n",
    "            nn.Conv2d(in_channels=3, out_channels=18, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(), nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "\n",
    "            # 2\n",
    "            nn.Conv2d(in_channels=18, out_channels=36, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(), nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "\n",
    "            # 3\n",
    "            nn.Conv2d(in_channels=36, out_channels=72, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 4\n",
    "            nn.Conv2d(in_channels=72, out_channels=72, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(), nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "\n",
    "            # 5\n",
    "            nn.Conv2d(in_channels=72, out_channels=144, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 6\n",
    "            nn.Conv2d(in_channels=144, out_channels=144, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(), nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "        )\n",
    "\n",
    "        # FC layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(144 * 4 * 4, 144 * 4 * 4), nn.ReLU(),\n",
    "            nn.Linear(144 * 4 * 4, 500), nn.ReLU(),\n",
    "            nn.Linear(500, 2)\n",
    "        )\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self, x):\n",
    "        x = self.convolutional_layers(x)\n",
    "        x = x.view(-1, 144*4*4)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_from = 1\n",
    "decoder_attempt_1 = ConvNet()\n",
    "decoder_attempt_1.cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(decoder_attempt_1.parameters(), \n",
    "                            lr=0.01,\n",
    "                            momentum=0.9, \n",
    "                            weight_decay=5e-4)\n",
    "\n",
    "\n",
    "# optimizer = optim.Adam(decoder.parameters(), lr=0.01)\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "best_loss = math.inf\n",
    "best_accuracy = 0.0\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print('\\n', '-'*10, ' Epoch: {} '.format(epoch), '-'*10)\n",
    "#     scheduler.step()\n",
    "    decoder_attempt_1.train() # set training mode\n",
    "    \n",
    "    train_dataset_size = 0\n",
    "    correct_train_preds = 0\n",
    "    running_train_loss = 0.0\n",
    "    \n",
    "    # training\n",
    "    for inputs, labels, ids in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward prop\n",
    "        outputs = decoder_attempt_1(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # metrics\n",
    "        running_train_loss += loss.item() * inputs.size(0)\n",
    "        correct_train_preds += torch.sum(preds == labels.data).item()\n",
    "        train_dataset_size += len(labels)\n",
    "        \n",
    "        \n",
    "    epoch_train_loss = running_train_loss / train_dataset_size\n",
    "    epoch_train_accuracy = (correct_train_preds*1.0) / train_dataset_size\n",
    "    \n",
    "    print('Training Loss: {}, Accuracy: {}'.format(epoch_train_loss, epoch_train_accuracy))\n",
    "    \n",
    "    \n",
    "    # validation\n",
    "    \n",
    "    decoder_attempt_1.eval() # set eval mode\n",
    "    \n",
    "    valid_dataset_size = 0\n",
    "    correct_valid_preds = 0\n",
    "    running_valid_loss = 0.0\n",
    "    \n",
    "    for inputs, labels, ids in valid_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = decoder_attempt_1(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        running_valid_loss += loss.item() * inputs.size(0)\n",
    "        correct_valid_preds += torch.sum(preds == labels.data).item()\n",
    "        valid_dataset_size += len(labels)\n",
    "\n",
    "    epoch_valid_loss = running_valid_loss / valid_dataset_size\n",
    "    epoch_valid_accuracy = (correct_valid_preds*1.0) / valid_dataset_size\n",
    "    \n",
    "    print('Validation Loss: {}, Accuracy: {}'.format(epoch_valid_loss, epoch_valid_accuracy))\n",
    "        \n",
    "        \n",
    "time_elapsed = time.time() - start_time\n",
    "\n",
    "print('Training completed in {}minutes {}secs'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ----------  Epoch: 0  ----------\n",
      "Training Loss: 0.6934774831031455, Accuracy: 0.49534345896618537\n",
      "Validation Loss: 0.693892868839821, Accuracy: 0.490622655663916\n",
      "\n",
      " ----------  Epoch: 1  ----------\n",
      "Training Loss: 0.6914611503559169, Accuracy: 0.5238452403275204\n",
      "Validation Loss: 0.6852969962318232, Accuracy: 0.5593898474618655\n",
      "\n",
      " ----------  Epoch: 2  ----------\n",
      "Training Loss: 0.6539089888123782, Accuracy: 0.6129758109881868\n",
      "Validation Loss: 0.6301940546151429, Accuracy: 0.6589147286821705\n",
      "\n",
      " ----------  Epoch: 3  ----------\n",
      "Training Loss: 0.6230093944489237, Accuracy: 0.6578536158509907\n",
      "Validation Loss: 0.6281935878770594, Accuracy: 0.646911727931983\n",
      "\n",
      " ----------  Epoch: 4  ----------\n",
      "Training Loss: 0.591485464391846, Accuracy: 0.6866679167447965\n",
      "Validation Loss: 0.5711549117613447, Accuracy: 0.7054263565891473\n",
      "\n",
      " ----------  Epoch: 5  ----------\n",
      "Training Loss: 0.5571032935876058, Accuracy: 0.7181073817113569\n",
      "Validation Loss: 0.5511538272247997, Accuracy: 0.7209302325581395\n",
      "\n",
      " ----------  Epoch: 6  ----------\n",
      "Training Loss: 0.5161620830213944, Accuracy: 0.744984061503844\n",
      "Validation Loss: 0.5079197156545548, Accuracy: 0.7554388597149287\n",
      "\n",
      " ----------  Epoch: 7  ----------\n",
      "Training Loss: 0.48354529181423245, Accuracy: 0.7672979561222576\n",
      "Validation Loss: 0.5015703535849049, Accuracy: 0.7566891722930733\n",
      "\n",
      " ----------  Epoch: 8  ----------\n",
      "Training Loss: 0.43465529312654705, Accuracy: 0.7996749796862304\n",
      "Validation Loss: 0.44901826925562693, Accuracy: 0.8014503625906476\n",
      "\n",
      " ----------  Epoch: 9  ----------\n",
      "Training Loss: 0.4107209218037159, Accuracy: 0.8110506906681667\n",
      "Validation Loss: 0.43786025010040025, Accuracy: 0.7904476119029757\n",
      "\n",
      " ----------  Epoch: 10  ----------\n",
      "Training Loss: 0.38360321388944135, Accuracy: 0.8249265579098694\n",
      "Validation Loss: 0.39116640753017484, Accuracy: 0.821955488872218\n",
      "\n",
      " ----------  Epoch: 11  ----------\n",
      "Training Loss: 0.3581463385819718, Accuracy: 0.8387399212450778\n",
      "Validation Loss: 0.3560574034521895, Accuracy: 0.8402100525131283\n",
      "\n",
      " ----------  Epoch: 12  ----------\n",
      "Training Loss: 0.3361543642222058, Accuracy: 0.8506781673854615\n",
      "Validation Loss: 0.35231015919357217, Accuracy: 0.8397099274818705\n",
      "\n",
      " ----------  Epoch: 13  ----------\n",
      "Training Loss: 0.3165653889939534, Accuracy: 0.8625539096193512\n",
      "Validation Loss: 0.3401505103794507, Accuracy: 0.8479619904976244\n",
      "\n",
      " ----------  Epoch: 14  ----------\n",
      "Training Loss: 0.2943220275686312, Accuracy: 0.8724920307519219\n",
      "Validation Loss: 0.31199976005951263, Accuracy: 0.8604651162790697\n",
      "\n",
      " ----------  Epoch: 15  ----------\n",
      "Training Loss: 0.2866863430382736, Accuracy: 0.876117257328583\n",
      "Validation Loss: 0.3098420090200186, Accuracy: 0.8684671167791947\n",
      "\n",
      " ----------  Epoch: 16  ----------\n",
      "Training Loss: 0.2678387517485442, Accuracy: 0.8862428901806363\n",
      "Validation Loss: 0.296210680750347, Accuracy: 0.8667166791697924\n",
      "\n",
      " ----------  Epoch: 17  ----------\n",
      "Training Loss: 0.26066752217271805, Accuracy: 0.8895555972248266\n",
      "Validation Loss: 0.26399847817409033, Accuracy: 0.8914728682170543\n",
      "\n",
      " ----------  Epoch: 18  ----------\n",
      "Training Loss: 0.23967793492687756, Accuracy: 0.8989311831989499\n",
      "Validation Loss: 0.28396722083391024, Accuracy: 0.8802200550137534\n",
      "\n",
      " ----------  Epoch: 19  ----------\n",
      "Training Loss: 0.23786017094167206, Accuracy: 0.8977436089755609\n",
      "Validation Loss: 0.30629660813293924, Accuracy: 0.8664666166541636\n",
      "\n",
      " ----------  Epoch: 20  ----------\n",
      "Training Loss: 0.22430499757600655, Accuracy: 0.90299393712107\n",
      "Validation Loss: 0.2862907260410426, Accuracy: 0.881220305076269\n",
      "\n",
      " ----------  Epoch: 21  ----------\n",
      "Training Loss: 0.21487367249006809, Accuracy: 0.9081192574535909\n",
      "Validation Loss: 0.2684719808759258, Accuracy: 0.881220305076269\n",
      "\n",
      " ----------  Epoch: 22  ----------\n",
      "Training Loss: 0.2033242169220199, Accuracy: 0.9124320270016876\n",
      "Validation Loss: 0.2663391673973305, Accuracy: 0.8799699924981246\n",
      "\n",
      " ----------  Epoch: 23  ----------\n",
      "Training Loss: 0.20518864364994788, Accuracy: 0.9131195699731233\n",
      "Validation Loss: 0.27138569971179033, Accuracy: 0.8772193048262066\n",
      "\n",
      " ----------  Epoch: 24  ----------\n",
      "Training Loss: 0.1958061741310266, Accuracy: 0.9178698668666792\n",
      "Validation Loss: 0.2607703982673308, Accuracy: 0.8849712428107027\n",
      "\n",
      " ----------  Epoch: 25  ----------\n",
      "Training Loss: 0.18782326378753927, Accuracy: 0.9225576598537408\n",
      "Validation Loss: 0.2608764859088274, Accuracy: 0.8879719929982496\n",
      "\n",
      " ----------  Epoch: 26  ----------\n",
      "Training Loss: 0.18000205530081773, Accuracy: 0.9239327457966123\n",
      "Validation Loss: 0.2752320810753752, Accuracy: 0.8792198049512379\n",
      "\n",
      " ----------  Epoch: 27  ----------\n",
      "Training Loss: 0.17700788573700083, Accuracy: 0.9246827926745421\n",
      "Validation Loss: 0.2539871757955782, Accuracy: 0.8954738684671167\n",
      "\n",
      " ----------  Epoch: 28  ----------\n",
      "Training Loss: 0.16709729627029055, Accuracy: 0.931433214575911\n",
      "Validation Loss: 0.2707852510570973, Accuracy: 0.894973743435859\n",
      "\n",
      " ----------  Epoch: 29  ----------\n",
      "Training Loss: 0.15972233663738442, Accuracy: 0.9341208825551597\n",
      "Validation Loss: 0.25576182397999564, Accuracy: 0.8954738684671167\n",
      "\n",
      " ----------  Epoch: 30  ----------\n",
      "Training Loss: 0.15715810973682703, Accuracy: 0.9344334020876305\n",
      "Validation Loss: 0.2905313388262131, Accuracy: 0.8982245561390347\n",
      "\n",
      " ----------  Epoch: 31  ----------\n",
      "Training Loss: 0.15384153790291835, Accuracy: 0.9361210075629727\n",
      "Validation Loss: 0.29135541133088866, Accuracy: 0.8847211802950737\n",
      "\n",
      " ----------  Epoch: 32  ----------\n",
      "Training Loss: 0.14814441140848916, Accuracy: 0.9393712107006688\n",
      "Validation Loss: 0.2583201348736871, Accuracy: 0.8904726181545386\n",
      "\n",
      " ----------  Epoch: 33  ----------\n",
      "Training Loss: 0.14144570455704863, Accuracy: 0.9419963747734234\n",
      "Validation Loss: 0.2922650161654569, Accuracy: 0.8917229307326832\n",
      "\n",
      " ----------  Epoch: 34  ----------\n",
      "Training Loss: 0.14162052424252125, Accuracy: 0.9423713982123882\n",
      "Validation Loss: 0.2657518950990809, Accuracy: 0.8899724931232809\n",
      "\n",
      " ----------  Epoch: 35  ----------\n",
      "Training Loss: 0.1347005904164193, Accuracy: 0.9449965622851428\n",
      "Validation Loss: 0.2681663805550413, Accuracy: 0.8879719929982496\n",
      "\n",
      " ----------  Epoch: 36  ----------\n",
      "Training Loss: 0.13178594414424402, Accuracy: 0.9451215700981311\n",
      "Validation Loss: 0.2654637472350528, Accuracy: 0.8969742435608902\n",
      "\n",
      " ----------  Epoch: 37  ----------\n",
      "Training Loss: 0.13294415551633773, Accuracy: 0.9449340583786486\n",
      "Validation Loss: 0.24603160250318082, Accuracy: 0.9014753688422106\n",
      "\n",
      " ----------  Epoch: 38  ----------\n",
      "Training Loss: 0.12242431452653729, Accuracy: 0.9509344334020876\n",
      "Validation Loss: 0.2690929343668214, Accuracy: 0.8964741185296324\n",
      "\n",
      " ----------  Epoch: 39  ----------\n",
      "Training Loss: 0.11970218805396801, Accuracy: 0.9521845115319707\n",
      "Validation Loss: 0.27961090224613155, Accuracy: 0.8944736184046012\n",
      "\n",
      " ----------  Epoch: 40  ----------\n",
      "Training Loss: 0.1178957816287647, Accuracy: 0.9525595349709357\n",
      "Validation Loss: 0.28202059883256353, Accuracy: 0.8952238059514879\n",
      "\n",
      " ----------  Epoch: 41  ----------\n",
      "Training Loss: 0.11712672188482223, Accuracy: 0.9515594724670292\n",
      "Validation Loss: 0.28737544728744385, Accuracy: 0.8969742435608902\n",
      "\n",
      " ----------  Epoch: 42  ----------\n",
      "Training Loss: 0.11440897438423925, Accuracy: 0.9552472029501844\n",
      "Validation Loss: 0.31272894909796817, Accuracy: 0.8772193048262066\n",
      "\n",
      " ----------  Epoch: 43  ----------\n",
      "Training Loss: 0.11306221885985603, Accuracy: 0.9553097068566786\n",
      "Validation Loss: 0.3203040369080436, Accuracy: 0.8864716179044762\n",
      "\n",
      " ----------  Epoch: 44  ----------\n",
      "Training Loss: 0.1151187840423753, Accuracy: 0.9547471716982311\n",
      "Validation Loss: 0.2920542161564256, Accuracy: 0.8844711177794449\n",
      "\n",
      " ----------  Epoch: 45  ----------\n",
      "Training Loss: 0.11285705272350709, Accuracy: 0.9555597224826552\n",
      "Validation Loss: 0.3252241915838514, Accuracy: 0.8714678669667417\n",
      "\n",
      " ----------  Epoch: 46  ----------\n",
      "Training Loss: 0.1047699780035346, Accuracy: 0.9573098318644915\n",
      "Validation Loss: 0.29645376469335366, Accuracy: 0.8924731182795699\n",
      "\n",
      " ----------  Epoch: 47  ----------\n",
      "Training Loss: 0.09484912511616754, Accuracy: 0.9622476404775299\n",
      "Validation Loss: 0.2893810011008645, Accuracy: 0.8927231807951987\n",
      "\n",
      " ----------  Epoch: 48  ----------\n",
      "Training Loss: 0.09442183828289058, Accuracy: 0.9648728045502843\n",
      "Validation Loss: 0.2904539037478152, Accuracy: 0.8942235558889723\n",
      "\n",
      " ----------  Epoch: 49  ----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.09743307498561062, Accuracy: 0.9604350271891994\n",
      "Validation Loss: 0.2694332707327704, Accuracy: 0.8844711177794449\n",
      "Training completed in 21.0minutes 12.225642681121826secs\n"
     ]
    }
   ],
   "source": [
    "epoch_from = 1\n",
    "decoder_attempt_2 = ConvNet()\n",
    "decoder_attempt_2.cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(decoder_attempt_2.parameters(), \n",
    "                            lr=0.01,\n",
    "                            momentum=0.9, \n",
    "                            weight_decay=5e-4)\n",
    "\n",
    "\n",
    "# optimizer = optim.Adam(decoder.parameters(), lr=0.01)\n",
    "\n",
    "# scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "best_loss = math.inf\n",
    "best_accuracy = 0.0\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print('\\n', '-'*10, ' Epoch: {} '.format(epoch), '-'*10)\n",
    "#     scheduler.step()\n",
    "    decoder_attempt_2.train() # set training mode\n",
    "    \n",
    "    train_dataset_size = 0\n",
    "    correct_train_preds = 0\n",
    "    running_train_loss = 0.0\n",
    "    \n",
    "    # training\n",
    "    for inputs, labels, ids in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward prop\n",
    "        outputs = decoder_attempt_2(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # metrics\n",
    "        running_train_loss += loss.item() * inputs.size(0)\n",
    "        correct_train_preds += torch.sum(preds == labels.data).item()\n",
    "        train_dataset_size += len(labels)\n",
    "        \n",
    "        \n",
    "    epoch_train_loss = running_train_loss / train_dataset_size\n",
    "    epoch_train_accuracy = (correct_train_preds*1.0) / train_dataset_size\n",
    "    \n",
    "    print('Training Loss: {}, Accuracy: {}'.format(epoch_train_loss, epoch_train_accuracy))\n",
    "    \n",
    "    \n",
    "    # validation\n",
    "    \n",
    "    decoder_attempt_2.eval() # set eval mode\n",
    "    \n",
    "    valid_dataset_size = 0\n",
    "    correct_valid_preds = 0\n",
    "    running_valid_loss = 0.0\n",
    "    \n",
    "    for inputs, labels, ids in valid_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = decoder_attempt_2(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        running_valid_loss += loss.item() * inputs.size(0)\n",
    "        correct_valid_preds += torch.sum(preds == labels.data).item()\n",
    "        valid_dataset_size += len(labels)\n",
    "        \n",
    "    epoch_valid_loss = running_valid_loss / valid_dataset_size\n",
    "    epoch_valid_accuracy = (correct_valid_preds*1.0) / valid_dataset_size\n",
    "    \n",
    "    print('Validation Loss: {}, Accuracy: {}'.format(epoch_valid_loss, epoch_valid_accuracy))\n",
    "        \n",
    "        \n",
    "time_elapsed = time.time() - start_time\n",
    "\n",
    "print('Training completed in {}minutes {}secs'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3rd attempt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ----------  Epoch: 0  ----------\n",
      "Training Loss: 0.6935378930979844, Accuracy: 0.5007187949246827\n",
      "Validation Loss: 0.6940448439309048, Accuracy: 0.4983745936484121\n",
      "\n",
      " ----------  Epoch: 1  ----------\n",
      "Training Loss: 0.6934254142118772, Accuracy: 0.5034689668104256\n",
      "Validation Loss: 0.6919998350248363, Accuracy: 0.5016254063515879\n",
      "\n",
      " ----------  Epoch: 2  ----------\n",
      "Training Loss: 0.6910999782667048, Accuracy: 0.5161572598287393\n",
      "Validation Loss: 0.6932700010918772, Accuracy: 0.4983745936484121\n",
      "\n",
      " ----------  Epoch: 3  ----------\n",
      "Training Loss: 0.6935032305437905, Accuracy: 0.5029064316519782\n",
      "Validation Loss: 0.6936783038934906, Accuracy: 0.5016254063515879\n",
      "\n",
      " ----------  Epoch: 4  ----------\n",
      "Training Loss: 0.6823910699575289, Accuracy: 0.5452215763485218\n",
      "Validation Loss: 0.6371076867323453, Accuracy: 0.6384096024006002\n",
      "\n",
      " ----------  Epoch: 5  ----------\n",
      "Training Loss: 0.6355473335642123, Accuracy: 0.6447902993937121\n",
      "Validation Loss: 0.6095011945693724, Accuracy: 0.6721680420105026\n",
      "\n",
      " ----------  Epoch: 6  ----------\n",
      "Training Loss: 0.61442166743066, Accuracy: 0.6656666041627601\n",
      "Validation Loss: 0.5953088115083781, Accuracy: 0.6896724181045262\n",
      "\n",
      " ----------  Epoch: 7  ----------\n",
      "Training Loss: 0.5832455769703756, Accuracy: 0.6982311394462154\n",
      "Validation Loss: 0.5960468218963901, Accuracy: 0.6796699174793699\n",
      "\n",
      " ----------  Epoch: 8  ----------\n",
      "Training Loss: 0.5615819587589197, Accuracy: 0.7114194637164822\n",
      "Validation Loss: 0.5631548761725277, Accuracy: 0.7046761690422606\n",
      "\n",
      " ----------  Epoch: 9  ----------\n",
      "Training Loss: 0.5342632082560873, Accuracy: 0.730545659103694\n",
      "Validation Loss: 0.5189882548012177, Accuracy: 0.7454363590897725\n",
      "\n",
      " ----------  Epoch: 10  ----------\n",
      "Training Loss: 0.5106698557305123, Accuracy: 0.7477342333895869\n",
      "Validation Loss: 0.4890840353817307, Accuracy: 0.7669417354338585\n",
      "\n",
      " ----------  Epoch: 11  ----------\n",
      "Training Loss: 0.4674074406720406, Accuracy: 0.7769860616288518\n",
      "Validation Loss: 0.4596211816347012, Accuracy: 0.7871967991997999\n",
      "\n",
      " ----------  Epoch: 12  ----------\n",
      "Training Loss: 0.44307820360447003, Accuracy: 0.7929870616913557\n",
      "Validation Loss: 0.4404455329826934, Accuracy: 0.7916979244811203\n",
      "\n",
      " ----------  Epoch: 13  ----------\n",
      "Training Loss: 0.4085673815541375, Accuracy: 0.8151759484967811\n",
      "Validation Loss: 0.40397872383608224, Accuracy: 0.8099524881220305\n",
      "\n",
      " ----------  Epoch: 14  ----------\n",
      "Training Loss: 0.3880035167757783, Accuracy: 0.8246765422838928\n",
      "Validation Loss: 0.3674814228520867, Accuracy: 0.8337084271067767\n",
      "\n",
      " ----------  Epoch: 15  ----------\n",
      "Training Loss: 0.36605262191185794, Accuracy: 0.8338021126320395\n",
      "Validation Loss: 0.39320538193918997, Accuracy: 0.8282070517629407\n",
      "\n",
      " ----------  Epoch: 16  ----------\n",
      "Training Loss: 0.3403413932828935, Accuracy: 0.8505531595724732\n",
      "Validation Loss: 0.4536027691086461, Accuracy: 0.7834458614653663\n",
      "\n",
      " ----------  Epoch: 17  ----------\n",
      "Training Loss: 0.32701008949952465, Accuracy: 0.8553034564660291\n",
      "Validation Loss: 0.3673896720243055, Accuracy: 0.8319579894973743\n",
      "\n",
      " ----------  Epoch: 18  ----------\n",
      "Training Loss: 0.3087765659397189, Accuracy: 0.8686167885492844\n",
      "Validation Loss: 0.3784378432175254, Accuracy: 0.835458864716179\n",
      "\n",
      " ----------  Epoch: 19  ----------\n",
      "Training Loss: 0.2932495003741595, Accuracy: 0.8735545971623226\n",
      "Validation Loss: 0.33951066878176417, Accuracy: 0.8532133033258315\n",
      "\n",
      " ----------  Epoch: 20  ----------\n",
      "Training Loss: 0.27513915971708536, Accuracy: 0.8819301206325395\n",
      "Validation Loss: 0.3125105188485532, Accuracy: 0.8559639909977494\n",
      "\n",
      " ----------  Epoch: 21  ----------\n",
      "Training Loss: 0.26271743236336753, Accuracy: 0.8868054253390837\n",
      "Validation Loss: 0.27051445330551604, Accuracy: 0.8834708677169293\n",
      "\n",
      " ----------  Epoch: 22  ----------\n",
      "Training Loss: 0.25168890347275275, Accuracy: 0.8907431714482155\n",
      "Validation Loss: 0.29007873104554055, Accuracy: 0.8722180545136284\n",
      "\n",
      " ----------  Epoch: 23  ----------\n",
      "Training Loss: 0.2433361077082888, Accuracy: 0.896556034752172\n",
      "Validation Loss: 0.2846798864296896, Accuracy: 0.8737184296074019\n",
      "\n",
      " ----------  Epoch: 24  ----------\n",
      "Training Loss: 0.23343666009705055, Accuracy: 0.9013063316457278\n",
      "Validation Loss: 0.34202681933456314, Accuracy: 0.8452113028257064\n",
      "\n",
      " ----------  Epoch: 25  ----------\n",
      "Training Loss: 0.2245822985944647, Accuracy: 0.9040565035314707\n",
      "Validation Loss: 0.2712089182675317, Accuracy: 0.8819704926231557\n",
      "\n",
      " ----------  Epoch: 26  ----------\n",
      "Training Loss: 0.21595108656594436, Accuracy: 0.909369335583474\n",
      "Validation Loss: 0.28010859016061695, Accuracy: 0.881220305076269\n",
      "\n",
      " ----------  Epoch: 27  ----------\n",
      "Training Loss: 0.20454390900709815, Accuracy: 0.9128695543471467\n",
      "Validation Loss: 0.2742516700775512, Accuracy: 0.8832208052013003\n",
      "\n",
      " ----------  Epoch: 28  ----------\n",
      "Training Loss: 0.20063193963612533, Accuracy: 0.9144321520095006\n",
      "Validation Loss: 0.3031042456574725, Accuracy: 0.8822205551387847\n",
      "\n",
      " ----------  Epoch: 29  ----------\n",
      "Training Loss: 0.19232460564170392, Accuracy: 0.9166197887367961\n",
      "Validation Loss: 0.2704751266035684, Accuracy: 0.8937234308577144\n",
      "\n",
      " ----------  Epoch: 30  ----------\n",
      "Training Loss: 0.1922538092105983, Accuracy: 0.9189324332770799\n",
      "Validation Loss: 0.2990584042809194, Accuracy: 0.8764691172793199\n",
      "\n",
      " ----------  Epoch: 31  ----------\n",
      "Training Loss: 0.1807171916434881, Accuracy: 0.9259328708044253\n",
      "Validation Loss: 0.3300439543010414, Accuracy: 0.8749687421855464\n",
      "\n",
      " ----------  Epoch: 32  ----------\n",
      "Training Loss: 0.18255785445796974, Accuracy: 0.9241827614225889\n",
      "Validation Loss: 0.2732779233343573, Accuracy: 0.8942235558889723\n",
      "\n",
      " ----------  Epoch: 33  ----------\n",
      "Training Loss: 0.168366005406118, Accuracy: 0.9286830426901681\n",
      "Validation Loss: 0.28299702572417157, Accuracy: 0.8882220555138785\n",
      "\n",
      " ----------  Epoch: 34  ----------\n",
      "Training Loss: 0.17347644411921093, Accuracy: 0.9284955309706857\n",
      "Validation Loss: 0.24786396891392895, Accuracy: 0.8909727431857964\n",
      "\n",
      " ----------  Epoch: 35  ----------\n",
      "Training Loss: 0.15757097378284665, Accuracy: 0.9363085192824552\n",
      "Validation Loss: 0.2954126189882739, Accuracy: 0.8899724931232809\n",
      "\n",
      " ----------  Epoch: 36  ----------\n",
      "Training Loss: 0.1513040601877475, Accuracy: 0.9376836052253266\n",
      "Validation Loss: 0.26114829320942173, Accuracy: 0.8884721180295074\n",
      "\n",
      " ----------  Epoch: 37  ----------\n",
      "Training Loss: 0.14853604817898602, Accuracy: 0.9372460778798675\n",
      "Validation Loss: 0.2983424029936937, Accuracy: 0.8834708677169293\n",
      "\n",
      " ----------  Epoch: 38  ----------\n",
      "Training Loss: 0.14597177621729487, Accuracy: 0.94149634352147\n",
      "Validation Loss: 0.2530801537693307, Accuracy: 0.8902225556389097\n",
      "\n",
      " ----------  Epoch: 39  ----------\n",
      "Training Loss: 0.14794235190299684, Accuracy: 0.9373085817863617\n",
      "Validation Loss: 0.26135427074212975, Accuracy: 0.8909727431857964\n",
      "\n",
      " ----------  Epoch: 40  ----------\n",
      "Training Loss: 0.1343169287529351, Accuracy: 0.9464966560410025\n",
      "Validation Loss: 0.3122228717775636, Accuracy: 0.8747186796699175\n",
      "\n",
      " ----------  Epoch: 41  ----------\n",
      "Training Loss: 0.1331419118498033, Accuracy: 0.9451215700981311\n",
      "Validation Loss: 0.2873635623910541, Accuracy: 0.8847211802950737\n",
      "\n",
      " ----------  Epoch: 42  ----------\n",
      "Training Loss: 0.1355129060560677, Accuracy: 0.9469966872929558\n",
      "Validation Loss: 0.3067764014780715, Accuracy: 0.8844711177794449\n",
      "\n",
      " ----------  Epoch: 43  ----------\n",
      "Training Loss: 0.13119147089782957, Accuracy: 0.9475592224514032\n",
      "Validation Loss: 0.30179050684936587, Accuracy: 0.8909727431857964\n",
      "\n",
      " ----------  Epoch: 44  ----------\n",
      "Training Loss: 0.12074572898000917, Accuracy: 0.9513094568410526\n",
      "Validation Loss: 0.2881424410711619, Accuracy: 0.8952238059514879\n",
      "\n",
      " ----------  Epoch: 45  ----------\n",
      "Training Loss: 0.1215409300166582, Accuracy: 0.9502468904306519\n",
      "Validation Loss: 0.320921803972488, Accuracy: 0.8802200550137534\n",
      "\n",
      " ----------  Epoch: 46  ----------\n",
      "Training Loss: 0.11595771761816125, Accuracy: 0.9540596287267954\n",
      "Validation Loss: 0.2955421279388298, Accuracy: 0.8864716179044762\n",
      "\n",
      " ----------  Epoch: 47  ----------\n",
      "Training Loss: 0.1221158405787662, Accuracy: 0.9515594724670292\n",
      "Validation Loss: 0.2796430043926475, Accuracy: 0.886721680420105\n",
      "\n",
      " ----------  Epoch: 48  ----------\n",
      "Training Loss: 0.11351042150188158, Accuracy: 0.9558097381086318\n",
      "Validation Loss: 0.29912075954255357, Accuracy: 0.8909727431857964\n",
      "\n",
      " ----------  Epoch: 49  ----------\n",
      "Training Loss: 0.10774230683205299, Accuracy: 0.9568723045190325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.31451180611559376, Accuracy: 0.8879719929982496\n",
      "\n",
      " ----------  Epoch: 50  ----------\n",
      "Training Loss: 0.11164885281927847, Accuracy: 0.9559972498281143\n",
      "Validation Loss: 0.28991744387266066, Accuracy: 0.8924731182795699\n",
      "\n",
      " ----------  Epoch: 51  ----------\n",
      "Training Loss: 0.10528427198262869, Accuracy: 0.9593099568723045\n",
      "Validation Loss: 0.3017193765476603, Accuracy: 0.8837209302325582\n",
      "\n",
      " ----------  Epoch: 52  ----------\n",
      "Training Loss: 0.11310673920300664, Accuracy: 0.9556847302956435\n",
      "Validation Loss: 0.3282301284411932, Accuracy: 0.8744686171542886\n",
      "\n",
      " ----------  Epoch: 53  ----------\n",
      "Training Loss: 0.1054898504507982, Accuracy: 0.9589974373398338\n",
      "Validation Loss: 0.3096133410692155, Accuracy: 0.8957239309827457\n",
      "\n",
      " ----------  Epoch: 54  ----------\n",
      "Training Loss: 0.09991392493154848, Accuracy: 0.9613100818801175\n",
      "Validation Loss: 0.2846294977301864, Accuracy: 0.8994748687171793\n",
      "\n",
      " ----------  Epoch: 55  ----------\n",
      "Training Loss: 0.09675430568730625, Accuracy: 0.9632477029814364\n",
      "Validation Loss: 0.37470767969905094, Accuracy: 0.883970992748187\n",
      "\n",
      " ----------  Epoch: 56  ----------\n",
      "Training Loss: 0.09711246605941792, Accuracy: 0.9621851365710357\n",
      "Validation Loss: 0.29188315901615586, Accuracy: 0.8932233058264566\n",
      "\n",
      " ----------  Epoch: 57  ----------\n",
      "Training Loss: 0.09632013458461715, Accuracy: 0.9611850740671292\n",
      "Validation Loss: 0.3244210391618157, Accuracy: 0.884221055263816\n",
      "\n",
      " ----------  Epoch: 58  ----------\n",
      "Training Loss: 0.0920766689159459, Accuracy: 0.9651853240827551\n",
      "Validation Loss: 0.31155097312437174, Accuracy: 0.8902225556389097\n",
      "\n",
      " ----------  Epoch: 59  ----------\n",
      "Training Loss: 0.09029942795939763, Accuracy: 0.9651853240827551\n",
      "Validation Loss: 0.3197711249550273, Accuracy: 0.8877219304826206\n",
      "\n",
      " ----------  Epoch: 60  ----------\n",
      "Training Loss: 0.08777464473009654, Accuracy: 0.9653103318957434\n",
      "Validation Loss: 0.38343522821584025, Accuracy: 0.881220305076269\n",
      "\n",
      " ----------  Epoch: 61  ----------\n",
      "Training Loss: 0.09296839494153152, Accuracy: 0.9638102381398838\n",
      "Validation Loss: 0.36730174983731206, Accuracy: 0.8804701175293823\n",
      "\n",
      " ----------  Epoch: 62  ----------\n",
      "Training Loss: 0.09359721834212216, Accuracy: 0.9629976873554598\n",
      "Validation Loss: 0.31324086802755186, Accuracy: 0.897474368592148\n",
      "\n",
      " ----------  Epoch: 63  ----------\n",
      "Training Loss: 0.08541252389405846, Accuracy: 0.966685417838615\n",
      "Validation Loss: 0.31195676910836984, Accuracy: 0.8912228057014253\n",
      "\n",
      " ----------  Epoch: 64  ----------\n",
      "Training Loss: 0.08196710257384843, Accuracy: 0.9679979998749921\n",
      "Validation Loss: 0.3269102907748662, Accuracy: 0.8907226806701676\n",
      "\n",
      " ----------  Epoch: 65  ----------\n",
      "Training Loss: 0.08474176558444438, Accuracy: 0.9677479842490155\n",
      "Validation Loss: 0.360612891880862, Accuracy: 0.881470367591898\n",
      "\n",
      " ----------  Epoch: 66  ----------\n",
      "Training Loss: 0.07995149374559556, Accuracy: 0.9693105819113694\n",
      "Validation Loss: 0.31956990149266185, Accuracy: 0.8899724931232809\n",
      "\n",
      " ----------  Epoch: 67  ----------\n",
      "Training Loss: 0.07824932882661588, Accuracy: 0.9704356522282642\n",
      "Validation Loss: 0.3574705849322238, Accuracy: 0.8772193048262066\n",
      "\n",
      " ----------  Epoch: 68  ----------\n",
      "Training Loss: 0.07962891220984977, Accuracy: 0.9689355584724045\n",
      "Validation Loss: 0.34727658333257305, Accuracy: 0.8859714928732183\n",
      "\n",
      " ----------  Epoch: 69  ----------\n",
      "Training Loss: 0.08171411924060899, Accuracy: 0.9680605037814863\n",
      "Validation Loss: 0.31242240875564414, Accuracy: 0.8872218054513629\n",
      "\n",
      " ----------  Epoch: 70  ----------\n",
      "Training Loss: 0.07457427130791461, Accuracy: 0.9706856678542409\n",
      "Validation Loss: 0.31064116069453157, Accuracy: 0.8844711177794449\n",
      "\n",
      " ----------  Epoch: 71  ----------\n",
      "Training Loss: 0.07512420647233282, Accuracy: 0.9708106756672292\n",
      "Validation Loss: 0.3618396007886497, Accuracy: 0.8822205551387847\n",
      "\n",
      " ----------  Epoch: 72  ----------\n",
      "Training Loss: 0.07679327265107115, Accuracy: 0.9698731170698168\n",
      "Validation Loss: 0.34874018118303396, Accuracy: 0.8914728682170543\n",
      "\n",
      " ----------  Epoch: 73  ----------\n",
      "Training Loss: 0.07505963367665959, Accuracy: 0.9708731795737233\n",
      "Validation Loss: 0.3824573357229264, Accuracy: 0.8827206801700425\n",
      "\n",
      " ----------  Epoch: 74  ----------\n",
      "Training Loss: 0.0741853296088013, Accuracy: 0.9716232264516532\n",
      "Validation Loss: 0.32356306122791534, Accuracy: 0.8904726181545386\n",
      "\n",
      " ----------  Epoch: 75  ----------\n",
      "Training Loss: 0.07463500481414395, Accuracy: 0.9718107381711357\n",
      "Validation Loss: 0.32393969558542685, Accuracy: 0.8934733683420856\n",
      "\n",
      " ----------  Epoch: 76  ----------\n",
      "Training Loss: 0.07231960784525371, Accuracy: 0.9728733045815363\n",
      "Validation Loss: 0.3606895193733344, Accuracy: 0.8917229307326832\n",
      "\n",
      " ----------  Epoch: 77  ----------\n",
      "Training Loss: 0.07453486780245473, Accuracy: 0.9714357147321707\n",
      "Validation Loss: 0.33167689743005324, Accuracy: 0.8897224306076519\n",
      "\n",
      " ----------  Epoch: 78  ----------\n",
      "Training Loss: 0.07012633579106202, Accuracy: 0.9726232889555597\n",
      "Validation Loss: 0.3267220452178207, Accuracy: 0.8939734933733433\n",
      "\n",
      " ----------  Epoch: 79  ----------\n",
      "Training Loss: 0.06990652405299748, Accuracy: 0.9740608788049253\n",
      "Validation Loss: 0.32892663525354565, Accuracy: 0.8807201800450113\n",
      "\n",
      " ----------  Epoch: 80  ----------\n",
      "Training Loss: 0.07756386407120644, Accuracy: 0.9699981248828051\n",
      "Validation Loss: 0.29455256695358656, Accuracy: 0.8874718679669917\n",
      "\n",
      " ----------  Epoch: 81  ----------\n",
      "Training Loss: 0.07162902110557615, Accuracy: 0.9728108006750422\n",
      "Validation Loss: 0.40821673115005075, Accuracy: 0.8797199299824956\n",
      "\n",
      " ----------  Epoch: 82  ----------\n",
      "Training Loss: 0.0772597620679499, Accuracy: 0.9708106756672292\n",
      "Validation Loss: 0.34293873848453643, Accuracy: 0.8847211802950737\n",
      "\n",
      " ----------  Epoch: 83  ----------\n",
      "Training Loss: 0.0657192277533165, Accuracy: 0.9741233827114195\n",
      "Validation Loss: 0.3658398461322482, Accuracy: 0.8857214303575894\n",
      "\n",
      " ----------  Epoch: 84  ----------\n",
      "Training Loss: 0.07047751177801095, Accuracy: 0.973935870991937\n",
      "Validation Loss: 0.3576501789585475, Accuracy: 0.8834708677169293\n",
      "\n",
      " ----------  Epoch: 85  ----------\n",
      "Training Loss: 0.06710180749914015, Accuracy: 0.974748421776361\n",
      "Validation Loss: 0.3663258766734502, Accuracy: 0.886721680420105\n",
      "\n",
      " ----------  Epoch: 86  ----------\n",
      "Training Loss: 0.06713449148736796, Accuracy: 0.9750609413088318\n",
      "Validation Loss: 0.36987122935812367, Accuracy: 0.8824706176544136\n",
      "\n",
      " ----------  Epoch: 87  ----------\n",
      "Training Loss: 0.07106666467025344, Accuracy: 0.9731858241140071\n",
      "Validation Loss: 0.3352158177894603, Accuracy: 0.8872218054513629\n",
      "\n",
      " ----------  Epoch: 88  ----------\n",
      "Training Loss: 0.0689444373623659, Accuracy: 0.973935870991937\n",
      "Validation Loss: 0.35729570162537755, Accuracy: 0.8822205551387847\n",
      "\n",
      " ----------  Epoch: 89  ----------\n",
      "Training Loss: 0.07141255471918037, Accuracy: 0.9738108631789487\n",
      "Validation Loss: 0.3559998833900632, Accuracy: 0.886721680420105\n",
      "\n",
      " ----------  Epoch: 90  ----------\n",
      "Training Loss: 0.0667662841318935, Accuracy: 0.9754359647477967\n",
      "Validation Loss: 0.37008959756967097, Accuracy: 0.8729682420605152\n",
      "\n",
      " ----------  Epoch: 91  ----------\n",
      "Training Loss: 0.0649920129992873, Accuracy: 0.9769360585036565\n",
      "Validation Loss: 0.33946031659923515, Accuracy: 0.8912228057014253\n",
      "\n",
      " ----------  Epoch: 92  ----------\n",
      "Training Loss: 0.06078954424500219, Accuracy: 0.9779986249140571\n",
      "Validation Loss: 0.3710832601190955, Accuracy: 0.875968992248062\n",
      "\n",
      " ----------  Epoch: 93  ----------\n",
      "Training Loss: 0.06974684395265322, Accuracy: 0.9746859178698669\n",
      "Validation Loss: 0.3481446876916983, Accuracy: 0.8882220555138785\n",
      "\n",
      " ----------  Epoch: 94  ----------\n",
      "Training Loss: 0.05953584168800429, Accuracy: 0.9766860428776799\n",
      "Validation Loss: 0.31730822842101447, Accuracy: 0.8924731182795699\n",
      "\n",
      " ----------  Epoch: 95  ----------\n",
      "Training Loss: 0.06544273131629982, Accuracy: 0.9749359334958435\n",
      "Validation Loss: 0.33305582573679754, Accuracy: 0.8819704926231557\n",
      "\n",
      " ----------  Epoch: 96  ----------\n",
      "Training Loss: 0.06137713150314126, Accuracy: 0.9769985624101506\n",
      "Validation Loss: 0.3482794920581077, Accuracy: 0.8854713678419605\n",
      "\n",
      " ----------  Epoch: 97  ----------\n",
      "Training Loss: 0.05938849525812682, Accuracy: 0.9782486405400338\n",
      "Validation Loss: 0.389444207826177, Accuracy: 0.8829707426856714\n",
      "\n",
      " ----------  Epoch: 98  ----------\n",
      "Training Loss: 0.0619891688923395, Accuracy: 0.9771860741296331\n",
      "Validation Loss: 0.3324052064783664, Accuracy: 0.89472368092023\n",
      "\n",
      " ----------  Epoch: 99  ----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.06238237089586393, Accuracy: 0.9763735233452091\n",
      "Validation Loss: 0.3327739164661723, Accuracy: 0.8952238059514879\n",
      "Training completed in 42.0minutes 4.715376138687134secs\n"
     ]
    }
   ],
   "source": [
    "epoch_from = 1\n",
    "decoder_attempt_3 = ConvNet()\n",
    "decoder_attempt_3.cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(decoder_attempt_3.parameters(), \n",
    "                            lr=0.01,\n",
    "                            momentum=0.9, \n",
    "                            weight_decay=5e-4)  # TODO: DIY implementation\n",
    "\n",
    "\n",
    "# optimizer = optim.Adam(decoder.parameters(), lr=0.01)\n",
    "\n",
    "# scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "best_loss = math.inf\n",
    "best_accuracy = 0.0\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print('\\n', '-'*10, ' Epoch: {} '.format(epoch), '-'*10)\n",
    "#     scheduler.step()\n",
    "    decoder_attempt_3.train() # set training mode\n",
    "    \n",
    "    train_dataset_size = 0\n",
    "    correct_train_preds = 0\n",
    "    running_train_loss = 0.0\n",
    "    \n",
    "    # training\n",
    "    for inputs, labels, ids in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward prop\n",
    "        outputs = decoder_attempt_3(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # metrics\n",
    "        running_train_loss += loss.item() * inputs.size(0)\n",
    "        correct_train_preds += torch.sum(preds == labels.data).item()\n",
    "        train_dataset_size += len(labels)\n",
    "        \n",
    "        \n",
    "    epoch_train_loss = running_train_loss / train_dataset_size\n",
    "    epoch_train_accuracy = (correct_train_preds*1.0) / train_dataset_size\n",
    "    \n",
    "    print('Training Loss: {}, Accuracy: {}'.format(epoch_train_loss, epoch_train_accuracy))\n",
    "    \n",
    "    \n",
    "    # validation\n",
    "    \n",
    "    decoder_attempt_3.eval() # set eval mode\n",
    "    \n",
    "    valid_dataset_size = 0\n",
    "    correct_valid_preds = 0\n",
    "    running_valid_loss = 0.0\n",
    "    \n",
    "    for inputs, labels, ids in valid_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = decoder_attempt_3(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        running_valid_loss += loss.item() * inputs.size(0)\n",
    "        correct_valid_preds += torch.sum(preds == labels.data).item()\n",
    "        valid_dataset_size += len(labels)\n",
    "\n",
    "    epoch_valid_loss = running_valid_loss / valid_dataset_size\n",
    "    epoch_valid_accuracy = (correct_valid_preds*1.0) / valid_dataset_size\n",
    "    \n",
    "    print('Validation Loss: {}, Accuracy: {}'.format(epoch_valid_loss, epoch_valid_accuracy))\n",
    "        \n",
    "        \n",
    "time_elapsed = time.time() - start_time\n",
    "\n",
    "print('Training completed in {}minutes {}secs'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(model, outfile_name):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    ids_and_predictions = dict()\n",
    "    \n",
    "    for inputs, labels, ids in test_loader:\n",
    "        ids_list = [x.split('/')[-1].split('.')[0] for x in ids]\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        preds_list = list(preds.cpu().numpy())\n",
    "        \n",
    "        count = 0\n",
    "\n",
    "        for (idx, pred) in zip(ids_list, preds_list):\n",
    "            idx = int(idx)\n",
    "            ids_and_predictions[idx] = idx_to_class[pred]\n",
    "        \n",
    "    print(len(ids_and_predictions))\n",
    "        \n",
    "    with open(outfile_name + '.csv', 'w') as out_file:\n",
    "        csv_writer = csv.writer(out_file)\n",
    "        csv_writer.writerow(['id', 'label'])\n",
    "        for idx in range(1, len(ids_and_predictions) + 1):\n",
    "            csv_writer.writerow([idx, ids_and_predictions[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_predictions(decoder, 'decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
