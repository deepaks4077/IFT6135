{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of question number 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(y, n_classes):\n",
    "    o = np.zeros(shape=(y.shape[0], n_classes))\n",
    "    for i in range(y.shape[0]):\n",
    "        o[i, int(y[i])] = 1\n",
    "    return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define NN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "class WeightInitializer(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def initialize_weights(self, dims):\n",
    "        pass\n",
    "\n",
    "class ZeroInitializer(WeightInitializer):\n",
    "    def initialize_weights(self, dims):\n",
    "        weights = []\n",
    "        for dim in dims:\n",
    "            weights.append((np.zeros(dim[0]), np.zeros(dim)))\n",
    "\n",
    "        return weights\n",
    "\n",
    "class NormalInitializer(WeightInitializer):\n",
    "    def initialize_weights(self, dims):\n",
    "        weights = []\n",
    "        for dim in dims:\n",
    "            weights.append((np.zeros(dim[0]), np.random.normal(0.0, 1.0, dim)))\n",
    "\n",
    "        return weights\n",
    "\n",
    "class GlorotInitializer(WeightInitializer):\n",
    "    def initialize_weights(self, dims):\n",
    "        weights = []\n",
    "        for dim in dims:\n",
    "            weight_range = np.sqrt(6. / (dim[0] + dim[1]))\n",
    "            weights.append((np.zeros(dim[0]), np.random.uniform(-weight_range, weight_range, size = dim)))\n",
    "\n",
    "        return weights\n",
    "\n",
    "# Currently this class only works with nets of exactly 2 hidden layers\n",
    "class NN:\n",
    "    def __init__(self, hidden_dims=(1024,2048), n_hidden=2, mode='train', weight_initer = GlorotInitializer(), \n",
    "        input_size = 2, output_size= 3):\n",
    "        print(\"hidden dims inside NN is \", hidden_dims)\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.n_hidden = n_hidden\n",
    "        self.weight_initer = weight_initer\n",
    "        \n",
    "        dims = [(hidden_dims[0], input_size), \n",
    "                (hidden_dims[1], hidden_dims[0]),\n",
    "                (output_size, hidden_dims[1])]\n",
    "\n",
    "        params = self.weight_initer.initialize_weights(dims)\n",
    "        self.b1, self.W1 = params[0]\n",
    "        self.b2, self.W2 = params[1]\n",
    "        self.b3, self.W3 = params[2]\n",
    "        self.parameters = [self.b1, self.W1, self.b2, self.W2, self.b3, self.W3]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 1:\n",
    "            x = x[None]\n",
    "        ha_1 = np.dot(x, self.W1.T) + self.b1\n",
    "        hs_1 = self.activation(ha_1)\n",
    "        \n",
    "        ha_2 = np.dot(hs_1, self.W2.T) + self.b2\n",
    "        hs_2 = self.activation(ha_2)\n",
    "        \n",
    "        oa = np.dot(hs_2, self.W3.T) + self.b3\n",
    "        os = self.softmax(oa, axis=1)\n",
    "        \n",
    "        return ha_1, hs_1, ha_2, hs_2, oa, os\n",
    "\n",
    "    def softmax(self, inp, axis = 1):\n",
    "        e_x = np.exp(inp - np.max(inp, axis=axis, keepdims=True))\n",
    "        return e_x / e_x.sum(axis=axis, keepdims=True)\n",
    "    \n",
    "    def backward(self, y, x, ha_1, hs_1, ha_2, hs_2, oa, os, weight_decay=0):\n",
    "        if len(x.shape) == 1:\n",
    "            x = x[None]\n",
    "            \n",
    "        bs = x.shape[0]\n",
    "        dl_doa = os - y\n",
    "        \n",
    "        dl_dW3 = np.dot(dl_doa.T, hs_2) / bs + weight_decay * self.W3\n",
    "        dl_db3 = dl_doa.mean(axis=0)\n",
    "        \n",
    "        dl_dhs_2 = np.dot(dl_doa, self.W3)\n",
    "        dl_dha_2 = (ha_2 > 0) * dl_dhs_2\n",
    "        \n",
    "        dl_dW2 = np.dot(dl_dha_2.T, hs_1) / bs + weight_decay * self.W2\n",
    "        dl_db2 = dl_dha_2.mean(axis=0)\n",
    "        \n",
    "        d1_dhs_1 = np.dot(dl_dha_2, self.W2)\n",
    "        dl_dha_1 = (ha_1 > 0) * d1_dhs_1\n",
    "        \n",
    "        dl_dW1 = np.dot(dl_dha_1.T, x) / bs + weight_decay * self.W1\n",
    "        dl_db1 = dl_dha_1.mean(axis=0)\n",
    "        \n",
    "        return dl_db1, dl_dW1, dl_db2, dl_dW2, dl_db3, dl_dW3\n",
    "    \n",
    "    def finite_diff(self, x, y, eps=1e-5):\n",
    "        p = self.parameters[3]\n",
    "        grad_fdiff = np.zeros(shape=p.shape)\n",
    "        for index, v in np.ndenumerate(p):\n",
    "            p[index] += eps\n",
    "            _, _, _, _, _, os_plus = self.forward(x)\n",
    "            loss_diff_plus = self.loss(os_plus, y)\n",
    "            p[index] -= 2*eps\n",
    "            _, _, _, _, _, os_minus = self.forward(x)\n",
    "            loss_diff_minus = self.loss(os_minus, y)\n",
    "            grad_fdiff[index] = (loss_diff_plus - loss_diff_minus) / float(2*eps)\n",
    "            p[index] += eps\n",
    "            \n",
    "        #gradients_finite_diff.append(grad_fdiff)\n",
    "        return grad_fdiff\n",
    "    \n",
    "    def activation(self, inp):\n",
    "        return (inp > 0) * inp\n",
    "\n",
    "    def loss(self, os, y):\n",
    "        return (y * (-np.log(os))).sum(axis=1).mean(axis=0)\n",
    "\n",
    "    def update(self, grads, learning_rate):\n",
    "        for p, grad in zip(self.parameters, grads):\n",
    "            updt = learning_rate * grad\n",
    "            p -= updt   \n",
    "        \n",
    "    def train(self, data, target, mb_size=100, learning_rate=1e-2, weight_decay=0.):\n",
    "        for i in range(data.shape[0] // mb_size):\n",
    "            xi = data[i*mb_size:(i+1)*mb_size]\n",
    "            yi = target[i*mb_size:(i+1)*mb_size]\n",
    "            ha_1, hs_1, ha_2, hs_2, oa, os = self.forward(xi)\n",
    "            average_grads = self.backward(yi, xi, ha_1, hs_1, ha_2, hs_2, oa, os, weight_decay)\n",
    "            average_loss = self.loss(os, yi)\n",
    "            self.update(average_grads, learning_rate)\n",
    "        return average_loss\n",
    "    \n",
    "    def test(self, x, y):\n",
    "        _, _, _, _, _, os = self.forward(x)\n",
    "        return self.loss(os, y), os.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Either put the fashionmnist folder in the current folder or uncomment the next line that will download it\n",
    "# !git clone https://github.com/zalandoresearch/fashion-mnist fashionmnist\n",
    "    \n",
    "from fashionmnist.utils import mnist_reader\n",
    "from random import shuffle\n",
    "\n",
    "X_train, y_train = mnist_reader.load_mnist('fashionmnist/data/fashion', kind='train')\n",
    "X_test, y_test = mnist_reader.load_mnist('fashionmnist/data/fashion', kind='t10k')\n",
    "\n",
    "indices = list(range(len(X_train)))\n",
    "shuffle(indices)\n",
    "\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# print(X_train[0])\n",
    "\n",
    "X_valid, y_valid = X_train[indices[55000:]], y_train[indices[55000:]]\n",
    "X_train, y_train = X_train[indices[:55000]], y_train[indices[:55000]]\n",
    "\n",
    "y_train_onehot = onehot(y_train, 10)\n",
    "y_valid_onehot = onehot(y_valid, 10)\n",
    "y_test_onehot = onehot(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test on fashionmnist with NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mlp = NN(hidden_dims = (512, 512), weight_initer = GlorotInitializer(), input_size = 784, output_size = 10)\n",
    "\n",
    "train_accuracies, train_losses = [], []\n",
    "valid_accuracies, valid_losses = [], []\n",
    "test_accuracies, test_losses = [], []\n",
    "num_epochs = 5\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    loss = mlp.train(X_train, y_train_onehot, mb_size=100)    \n",
    "    loss_train, pred_train = mlp.test(X_train, y_train_onehot)\n",
    "    loss_valid, pred_valid = mlp.test(X_valid, y_valid_onehot)\n",
    "    loss_test, pred_test = mlp.test(X_test, y_test_onehot)\n",
    "    valid_losses.append(loss_valid)\n",
    "    test_losses.append(loss_test)\n",
    "    valid_accuracies.append((pred_valid == y_valid).mean())\n",
    "    test_accuracies.append((pred_test == y_test).mean())\n",
    "    train_losses.append(loss_train)\n",
    "    train_accuracies.append((pred_train == y_train).mean())\n",
    "\n",
    "## Loss graph\n",
    "plt.figure(figsize=(12, 4))\n",
    "axis = plt.subplot(1, 2, 1)\n",
    "axis.plot(range(1, len(train_losses)+1), train_losses, label='train')\n",
    "axis.plot(range(1, len(valid_losses)+1), valid_losses, label='valid')\n",
    "axis.plot(range(1, len(test_losses)+1), test_losses, label='test')\n",
    "axis.legend()\n",
    "axis.set_ylabel('Loss')\n",
    "axis.set_xlabel('Epochs')\n",
    "\n",
    "## Accuracy graph\n",
    "axis = plt.subplot(1, 2, 2)\n",
    "axis.plot(range(1, len(train_accuracies)+1), train_accuracies, label='train')\n",
    "axis.plot(range(1, len(valid_accuracies)+1), valid_accuracies, label='valid')\n",
    "axis.plot(range(1, len(test_accuracies)+1), test_accuracies, label='test')\n",
    "axis.legend()\n",
    "axis.set_ylabel('Accuracy')\n",
    "axis.set_xlabel('Epochs')\n",
    "\n",
    "plt.title(\"FashionMNIST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len X train  50000\n",
      "len y train  50000\n",
      "len X valid  10000\n",
      "len y valid  10000\n",
      "len X test  10000\n",
      "len y test  10000\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "# mnist_data = mnist_reader.load_mnist('mnist/raw/', kind='train')\n",
    "mnist_data = np.load('../datasets/mnist.npy', encoding='latin1')\n",
    "\n",
    "X_train, y_train = mnist_data[0]\n",
    "X_valid, y_valid = mnist_data[1]\n",
    "X_test, y_test = mnist_data[2]\n",
    "\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_valid = X_valid.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "\n",
    "print('len X train ', len(X_train))\n",
    "print('len y train ', len(y_train))\n",
    "print('len X valid ', len(X_valid))\n",
    "print('len y valid ', len(y_valid))\n",
    "print('len X test ', len(X_test))\n",
    "print('len y test ', len(y_test))\n",
    "\n",
    "indices = list(range(len(X_train)))\n",
    "shuffle(indices)\n",
    "\n",
    "y_train_onehot = onehot(y_train, 10)\n",
    "y_valid_onehot = onehot(y_valid, 10)\n",
    "y_test_onehot = onehot(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden dims inside NN is  (512, 512)\n",
      "[0.0001, 1e-05, 5e-05, 5e-06, 3.3333333333333335e-05, 3.3333333333333333e-06, 2.5e-05, 2.5e-06, 2e-05, 2e-06]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "x1 = X_train[0]\n",
    "y1 = y_train_onehot[0]\n",
    "\n",
    "mlp = NN(hidden_dims = (512, 512), weight_initer = GlorotInitializer(), input_size = 784, output_size = 10)\n",
    "ha_1, hs_1, ha_2, hs_2, oa, os = mlp.forward(x1)\n",
    "\n",
    "# grab the true gradients for the weights of the first layer\n",
    "true_gradients = mlp.backward(y1, x1, ha_1, hs_1, ha_2, hs_2, oa, os)\n",
    "\n",
    "chosen_epsilon = []\n",
    "for k in range(1,6):\n",
    "    chosen_epsilon.append(1 / float(k * math.pow(10, 4)))\n",
    "    chosen_epsilon.append(1 / float(k * math.pow(10, 5)))\n",
    "\n",
    "print(chosen_epsilon)\n",
    "\n",
    "epsilon = chosen_epsilon[9]\n",
    "finite_diff_grad = mlp.finite_diff(x1,y1, eps=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512)\n",
      "[0.         0.00598306 0.00237779 0.         0.00432756 0.00063225\n",
      " 0.         0.         0.00816281 0.        ]\n",
      "(512, 512)\n",
      "[0.         0.00598306 0.00237779 0.         0.00432756 0.00063225\n",
      " 0.         0.         0.00816281 0.        ]\n",
      "max diff  1.7752397165129996e-10\n"
     ]
    }
   ],
   "source": [
    "print(true_gradients[3].shape)\n",
    "print(true_gradients[3].flatten()[:10])\n",
    "print(finite_diff_grad.shape)\n",
    "print(finite_diff_grad.flatten()[:10])\n",
    "\n",
    "difference = np.abs(finite_diff_grad.flatten()[:10] - true_gradients[3].flatten()[:10])\n",
    "max_diff = np.max(difference)\n",
    "\n",
    "print('max diff ', max_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute train/valid/test loss and accuracy and display training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = NN(hidden_dims = (64, 32), weight_initer = GlorotInitializer(), input_size = 784, output_size = 10)\n",
    "\n",
    "train_accuracies, train_losses = [], []\n",
    "valid_accuracies, valid_losses = [], []\n",
    "test_accuracies, test_losses = [], []\n",
    "num_epochs = 60\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    \n",
    "    loss = mlp.train(X_train, y_train_onehot, mb_size=100)\n",
    "    loss_train, pred_train = mlp.test(X_train, y_train_onehot)\n",
    "    loss_valid, pred_valid = mlp.test(X_valid, y_valid_onehot)\n",
    "    loss_test, pred_test = mlp.test(X_test, y_test_onehot)\n",
    "    \n",
    "    valid_losses.append(loss_valid)\n",
    "    test_losses.append(loss_test)\n",
    "    valid_accuracies.append((pred_valid == y_valid).mean())\n",
    "    test_accuracies.append((pred_test == y_test).mean())\n",
    "    train_losses.append(loss_train)\n",
    "    train_accuracies.append((pred_train == y_train).mean())\n",
    "\n",
    "## Loss figure\n",
    "plt.figure(figsize=(12, 4))\n",
    "axis = plt.subplot(1, 2, 1)\n",
    "axis.plot(range(1, len(train_losses)+1), train_losses, label='train')\n",
    "axis.plot(range(1, len(valid_losses)+1), valid_losses, label='valid')\n",
    "axis.plot(range(1, len(test_losses)+1), test_losses, label='test')\n",
    "axis.legend()\n",
    "axis.set_ylabel('Loss')\n",
    "axis.set_xlabel('Epochs')\n",
    "\n",
    "## Accuracy figure\n",
    "axis = plt.subplot(1, 2, 2)\n",
    "axis.plot(range(1, len(train_accuracies)+1), train_accuracies, label='train')\n",
    "axis.plot(range(1, len(valid_accuracies)+1), valid_accuracies, label='valid')\n",
    "axis.plot(range(1, len(test_accuracies)+1), test_accuracies, label='test')\n",
    "axis.legend()\n",
    "axis.set_ylabel('Accuracy')\n",
    "axis.set_xlabel('Epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valid_accuracies[59])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
