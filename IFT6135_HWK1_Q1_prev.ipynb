{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of question number 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(y, n_classes):\n",
    "    o = np.zeros(shape=(y.shape[0], n_classes))\n",
    "    for i in range(y.shape[0]):\n",
    "        o[i, int(y[i])] = 1\n",
    "    return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define NN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "def softmax(x, axis=1):\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=axis, keepdims=True)\n",
    "\n",
    "class WeightInitializer(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def initialize_weights(self, dims):\n",
    "        pass\n",
    "\n",
    "class ZeroInitializer(WeightInitializer):\n",
    "    def initialize_weights(self, dims):\n",
    "        weights = []\n",
    "        for dim in dims:\n",
    "            weights.append((np.zeros(dim[0]), np.zeros(dim)))\n",
    "\n",
    "        return weights\n",
    "\n",
    "class NormalInitializer(WeightInitializer):\n",
    "    def initialize_weights(self, dims):\n",
    "        weights = []\n",
    "        for dim in dims:\n",
    "            weights.append((np.zeros(dim[0]), np.random.randn(dim[0], dim[1])))\n",
    "\n",
    "        return weights\n",
    "\n",
    "class GlorotInitializer(WeightInitializer):\n",
    "    def initialize_weights(self, dims):\n",
    "        weights = []\n",
    "        for dim in dims:\n",
    "            weight_range = np.sqrt(6. / (dim[0] + dim[1]))\n",
    "            weights.append((np.zeros(dim[0]), np.random.uniform(-weight_range, weight_range, size = dim)))\n",
    "\n",
    "        return weights\n",
    "\n",
    "# Currently this class only works with nets of exactly 2 hidden layers\n",
    "class NN:\n",
    "    def __init__(self, hidden_dims=(1024,2048), n_hidden=2, mode='train', weight_initer = GlorotInitializer(), \n",
    "        input_size = 2, output_size= 3):\n",
    "        \n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.n_hidden = n_hidden\n",
    "        self.weight_initer = weight_initer\n",
    "        \n",
    "        dims = [(hidden_dims[0], input_size), \n",
    "                (hidden_dims[1], hidden_dims[0]),\n",
    "                (output_size, hidden_dims[1])]\n",
    "\n",
    "        params = self.weight_initer.initialize_weights(dims)\n",
    "        self.b1, self.W1 = params[0]\n",
    "        self.b2, self.W2 = params[1]\n",
    "        self.b3, self.W3 = params[2]\n",
    "\n",
    "        self.parameters = [self.b1, self.W1, self.b2, self.W2, self.b3, self.W3]\n",
    "        \n",
    "        print(\"W1 shape \", self.W1.shape)\n",
    "        print(\"b1 shape \", self.b1.shape)\n",
    "        print(\"W2 shape \", self.W2.shape)\n",
    "        print(\"b2 shape \", self.b2.shape)\n",
    "        print(\"W3 shape \", self.W3.shape)\n",
    "        print(\"b3 shape \", self.b3.shape)\n",
    "        \n",
    "        print(\"W1 shape \", self.W1.dtype)\n",
    "        print(\"b1 shape \", self.b1.dtype)\n",
    "        print(\"W2 shape \", self.W2.dtype)\n",
    "        print(\"b2 shape \", self.b2.dtype)\n",
    "        print(\"W3 shape \", self.W3.dtype)\n",
    "        print(\"b3 shape \", self.b3.dtype)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 1:\n",
    "            x = x[None]\n",
    "            \n",
    "        ha_1 = np.dot(x, self.W1.T) + self.b1\n",
    "        hs_1 = self.activation(ha_1)\n",
    "        \n",
    "        ha_2 = np.dot(hs_1, self.W2.T) + self.b2\n",
    "        hs_2 = self.activation(ha_2)\n",
    "        \n",
    "        oa = np.dot(hs_2, self.W3.T) + self.b3\n",
    "        os = softmax(oa, axis=1)\n",
    "        \n",
    "        return ha_1, hs_1, ha_2, hs_2, oa, os\n",
    "\n",
    "    def softmax(self, inp, axis = 1):\n",
    "        e_x = np.exp(inp - np.max(inp, axis=axis, keepdims=True))\n",
    "        return e_x / e_x.sum(axis=axis, keepdims=True)\n",
    "    \n",
    "    def backward(self, y, x, ha_1, hs_1, ha_2, hs_2, oa, os, weight_decay=0):\n",
    "        if len(x.shape) == 1:\n",
    "            x = x[None]\n",
    "        \n",
    "        bs = x.shape[0]\n",
    "        dl_doa = os - y\n",
    "        \n",
    "        dl_dW3 = np.dot(dl_doa.T, hs_2) / bs + weight_decay * self.W3\n",
    "        dl_db3 = dl_doa.mean(axis=0)\n",
    "        \n",
    "        dl_dhs_2 = np.dot(dl_doa, self.W3)\n",
    "        dl_dha_2 = (ha_2 > 0) * dl_dhs_2\n",
    "        \n",
    "        dl_dW2 = np.dot(dl_dha_2.T, hs_1) / bs + weight_decay * self.W2\n",
    "        dl_db2 = dl_dha_2.mean(axis=0)\n",
    "        \n",
    "        d1_dhs_1 = np.dot(dl_dha_2, self.W2)\n",
    "        dl_dha_1 = (ha_1 > 0) * d1_dhs_1\n",
    "        \n",
    "        dl_dW1 = np.dot(dl_dha_1.T, x) / bs + weight_decay * self.W1\n",
    "        dl_db1 = dl_dha_1.mean(axis=0)\n",
    "        \n",
    "        return dl_db1, dl_dW1, dl_db2, dl_dW2, dl_db3, dl_dW3\n",
    "    \n",
    "    def calc_finite_diff(self, y, x, eps=1, limit = 10):\n",
    "        ha_1, hs_1, ha_2, hs_2, oa, os = self.forward(x)\n",
    "        gradients_bprop = self.backward(y, x, ha_1, hs_1, ha_2, hs_2, oa, os)\n",
    "        loss = self.loss(os, y)\n",
    "        gradients_finite_diff = []\n",
    "        for idx, p in enumerate(self.parameters):\n",
    "            grad_fdiff = np.zeros(shape=p.shape)\n",
    "            cnt = 0\n",
    "            for index, v in np.ndenumerate(p):\n",
    "                prev_val = self.parameters[idx][index]\n",
    "                \n",
    "                ## Calculate loss after adding eps\n",
    "                p[index] += eps\n",
    "#                 if self.parameters[idx][index] == p[index]:\n",
    "#                     diff = self.parameters[idx][index] - prev_val\n",
    "#                     print(f\"valid: diff = {diff}\")\n",
    "                    \n",
    "                _, _, _, _, _, os = self.forward(x)\n",
    "                loss_diff_1 = self.loss(os, y)\n",
    "                \n",
    "                ## Calculate loss after subtracting eps\n",
    "                p[index] -= 2*eps\n",
    "                _, _, _, _, _, os = self.forward(x)\n",
    "                loss_diff_2 = self.loss(os, y)\n",
    "                \n",
    "                ## Calculate the finite difference\n",
    "                grad_fdiff[index] = (loss_diff_1 - loss_diff_2) / (2*eps)\n",
    "                \n",
    "                print(gradients_bprop[idx][index])\n",
    "                \n",
    "                ## Reset the gradient\n",
    "                p[index] += eps\n",
    "                \n",
    "                cnt += 1\n",
    "                if cnt >= limit:\n",
    "                    break\n",
    "                \n",
    "            gradients_finite_diff.append(grad_fdiff)\n",
    "        return gradients_finite_diff\n",
    "    \n",
    "    def activation(self, inp):\n",
    "        return (inp > 0) * inp\n",
    "\n",
    "    def loss(self, os, y):\n",
    "        return (y * (-np.log(os))).sum(axis=1).mean(axis=0)\n",
    "\n",
    "    def update(self, grads, learning_rate):\n",
    "        for p, grad in zip(self.parameters, grads):\n",
    "            updt = learning_rate * grad\n",
    "            p -= updt   \n",
    "        \n",
    "    def train(self, data, target, mb_size=100, learning_rate=1e-2, weight_decay=0.):\n",
    "        for i in range(data.shape[0] // mb_size):\n",
    "            xi = data[i*mb_size:(i+1)*mb_size]\n",
    "            yi = target[i*mb_size:(i+1)*mb_size]\n",
    "            ha_1, hs_1, ha_2, hs_2, oa, os = self.forward(xi)\n",
    "            average_grads = self.backward(yi, xi, ha_1, hs_1, ha_2, hs_2, oa, os, weight_decay)\n",
    "            average_loss = self.loss(os, yi)\n",
    "            self.update(average_grads, learning_rate)\n",
    "        return average_loss\n",
    "    \n",
    "    def test(self, x, y):\n",
    "        _, _, _, _, _, os = self.forward(x)\n",
    "        return self.loss(os, y), os.argmax(axis=1)\n",
    "    \n",
    "    def loop_fprop(self, x):\n",
    "        ha_1 = np.dot(self.W1, x) + self.b1\n",
    "        hs_1 = self.activation(ha_1)\n",
    "        \n",
    "        ha_2 = np.dot(self.W2, hs_1) + self.b2\n",
    "        hs_2 = self.activation(ha_2)\n",
    "        \n",
    "        oa = np.dot(self.W3, hs_2) + self.b3\n",
    "        os = self.softmax(oa, axis=0)\n",
    "        return ha_1, hs_1, ha_2, hs_2, oa, os\n",
    "        \n",
    "    def loop_bprop(self, y, x, ha_1, hs_1, ha_2, hs_2, oa, os, weight_decay=0):\n",
    "        dl_doa = os - y\n",
    "        \n",
    "        dl_dW3 = np.outer(dl_doa, hs_2) + weight_decay * self.W3\n",
    "        dl_db3 = dl_doa\n",
    "        \n",
    "        dl_dhs_2 = np.dot(self.W3.T, dl_doa)\n",
    "        dl_dha_2 = (ha_2 > 0) * dl_dhs_2\n",
    "        \n",
    "        dl_dW2 = np.outer(dl_dha_2, hs_1) + weight_decay * self.W2\n",
    "        dl_db2 = dl_dha_2\n",
    "        \n",
    "        dl_dhs_1 = np.dot(self.W2.T, dl_dha_2)\n",
    "        dl_dha_1 = (ha_1 > 0) * dl_dhs_1\n",
    "        \n",
    "        \n",
    "        dl_dW1 = np.outer(dl_dha_1, x) + weight_decay * self.W1\n",
    "        dl_db1 = dl_dha_1\n",
    "        \n",
    "        return dl_db1, dl_dW1, dl_db2, dl_dW2, dl_db3, dl_dW3\n",
    "    \n",
    "    def loop_loss(self, os, y):\n",
    "        return (y * (-np.log(os))).sum()\n",
    "    \n",
    "    def loop_finite_diff(self, y, x, eps=1e-5, limit = 10):\n",
    "        ha_1, hs_1, ha_2, hs_2, oa, os = self.loop_fprop(x)\n",
    "        gradients_bprop = self.loop_bprop(y, x, ha_1, hs_1, ha_2, hs_2, oa, os)\n",
    "        loss = self.loop_loss(os, y)\n",
    "        gradients_finite_diff = []\n",
    "        for idx, p in enumerate(self.parameters):\n",
    "            grad_fdiff = np.zeros(shape=p.shape)\n",
    "            cnt = 0\n",
    "            for index, v in np.ndenumerate(p):\n",
    "                prev_val = self.parameters[idx][index]\n",
    "                \n",
    "                ## Calculate loss after adding eps\n",
    "                p[index] += eps                    \n",
    "                _, _, _, _, _, os = self.loop_fprop(x)\n",
    "                loss_diff_1 = self.loop_loss(os, y)\n",
    "                \n",
    "                ## Calculate loss after subtracting eps\n",
    "                p[index] -= 2*eps\n",
    "                _, _, _, _, _, os = self.loop_fprop(x)\n",
    "                loss_diff_2 = self.loop_loss(os, y)\n",
    "                \n",
    "                ## Calculate the finite difference\n",
    "                grad_fdiff[index] = (loss_diff_1 - loss_diff_2) / (2*eps)\n",
    "                \n",
    "                ## Reset the gradient\n",
    "                p[index] += eps\n",
    "                \n",
    "                cnt += 1\n",
    "                if cnt >= limit:\n",
    "                    break\n",
    "                    \n",
    "#                 p[index] += eps\n",
    "#                 _, _, _, _, _, os = self.loop_fprop(x)\n",
    "#                 loss_diff = self.loop_loss(os, y)\n",
    "#                 grad_fdiff[index] = (loss_diff - loss) / eps\n",
    "#                 p[index] -= eps\n",
    "            gradients_finite_diff.append(grad_fdiff)\n",
    "        return gradients_finite_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len X train  50000\n",
      "len y train  50000\n",
      "len X valid  10000\n",
      "len y valid  10000\n",
      "len X test  10000\n",
      "len y test  10000\n",
      "(50000, 784)\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "mnist_data = np.load('mnist/mnist.pkl.npy', encoding='latin1')\n",
    "X_train, y_train = mnist_data[0]\n",
    "X_valid, y_valid = mnist_data[1]\n",
    "X_test, y_test = mnist_data[2]\n",
    "\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(mnist_data[0])\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_valid = X_valid.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "\n",
    "print('len X train ', len(X_train))\n",
    "print('len y train ', len(y_train))\n",
    "print('len X valid ', len(X_valid))\n",
    "print('len y valid ', len(y_valid))\n",
    "print('len X test ', len(X_test))\n",
    "print('len y test ', len(y_test))\n",
    "\n",
    "indices = list(range(len(X_train)))\n",
    "shuffle(indices)\n",
    "\n",
    "X_train = X_train / 255\n",
    "X_valid = X_valid / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "y_train_onehot = onehot(y_train, 10)\n",
    "y_valid_onehot = onehot(y_valid, 10)\n",
    "y_test_onehot = onehot(y_test, 10)\n",
    "\n",
    "print(X_train.shape)\n",
    "# print(X_train[0])\n",
    "# print(y_train_onehot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 shape  (512, 784)\n",
      "b1 shape  (512,)\n",
      "W2 shape  (512, 512)\n",
      "b2 shape  (512,)\n",
      "W3 shape  (10, 512)\n",
      "b3 shape  (10,)\n",
      "W1 shape  float64\n",
      "b1 shape  float64\n",
      "W2 shape  float64\n",
      "b2 shape  float64\n",
      "W3 shape  float64\n",
      "b3 shape  float64\n",
      "0.0\n",
      "-38.21388411813889\n",
      "-259.92340330982137\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "11.780651748963173\n",
      "0.0\n",
      "8.336520468890598\n",
      "-16.201868311342633\n",
      "0.0\n",
      "-10.463599135181816\n",
      "-3.689253789044054\n",
      "0.0\n",
      "21.38244298547148\n",
      "0.0\n",
      "0.0\n",
      "0.8930561683203645\n",
      "0.3801339347429114\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "-4.999749474652536\n",
      "-4.999999562742388\n",
      "-4.996822103746436\n",
      "-4.831273372401906\n",
      "-4.9999999999220055\n",
      "-4.998035917989991\n",
      "-4.999977280687211\n",
      "-4.988650149156907\n",
      "-4.986239922491004\n",
      "-4.199252216209615\n",
      "-5.109816974160759\n",
      "0.0\n",
      "-3.4392157306507114\n",
      "-1.6563879860222424\n",
      "0.0\n",
      "-0.2823677210245171\n",
      "-5.5494479692483365\n",
      "0.0\n",
      "-3.6012943966911757\n",
      "0.0\n",
      "matrix max difference with n = 0.25, max_diff = 2.420164424822787\n",
      "0.0\n",
      "-38.21388411813889\n",
      "-259.92340330982137\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "11.780651748963173\n",
      "0.0\n",
      "8.336520468890598\n",
      "-16.201868311342633\n",
      "0.0\n",
      "-10.463599135181816\n",
      "-3.689253789044054\n",
      "0.0\n",
      "21.38244298547148\n",
      "0.0\n",
      "0.0\n",
      "0.8930561683203645\n",
      "0.3801339347429114\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "-4.999749474652536\n",
      "-4.999999562742388\n",
      "-4.996822103746436\n",
      "-4.831273372401906\n",
      "-4.9999999999220055\n",
      "-4.998035917989991\n",
      "-4.999977280687211\n",
      "-4.988650149156907\n",
      "-4.986239922491004\n",
      "-4.199252216209615\n",
      "-5.109816974160759\n",
      "0.0\n",
      "-3.4392157306507114\n",
      "-1.6563879860222424\n",
      "0.0\n",
      "-0.2823677210245171\n",
      "-5.5494479692483365\n",
      "0.0\n",
      "-3.6012943966911757\n",
      "0.0\n",
      "matrix max difference with n = 0.5, max_diff = 2.420164424822787\n",
      "0.0\n",
      "-38.21388411813889\n",
      "-259.92340330982137\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "11.780651748963173\n",
      "0.0\n",
      "8.336520468890598\n",
      "-16.201868311342633\n",
      "0.0\n",
      "-10.463599135181816\n",
      "-3.689253789044054\n",
      "0.0\n",
      "21.38244298547148\n",
      "0.0\n",
      "0.0\n",
      "0.8930561683203645\n",
      "0.3801339347429114\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "-4.999749474652536\n",
      "-4.999999562742388\n",
      "-4.996822103746436\n",
      "-4.831273372401906\n",
      "-4.9999999999220055\n",
      "-4.998035917989991\n",
      "-4.999977280687211\n",
      "-4.988650149156907\n",
      "-4.986239922491004\n",
      "-4.199252216209615\n",
      "-5.109816974160759\n",
      "0.0\n",
      "-3.4392157306507114\n",
      "-1.6563879860222424\n",
      "0.0\n",
      "-0.2823677210245171\n",
      "-5.5494479692483365\n",
      "0.0\n",
      "-3.6012943966911757\n",
      "0.0\n",
      "matrix max difference with n = 1, max_diff = 2.420164424822787\n",
      "0.0\n",
      "-38.21388411813889\n",
      "-259.92340330982125\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "11.78065174896317\n",
      "0.0\n",
      "8.336520468890598\n",
      "-16.201868311342633\n",
      "0.0\n",
      "-10.463599135181815\n",
      "-3.689253789044054\n",
      "0.0\n",
      "21.38244298547148\n",
      "0.0\n",
      "0.0\n",
      "0.8930561683203643\n",
      "0.3801339347429113\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "-4.999749474652536\n",
      "-4.999999562742388\n",
      "-4.996822103746436\n",
      "-4.831273372401906\n",
      "-4.9999999999220055\n",
      "-4.998035917989991\n",
      "-4.999977280687211\n",
      "-4.988650149156907\n",
      "-4.986239922491004\n",
      "-4.199252216209615\n",
      "-5.109816974160759\n",
      "0.0\n",
      "-3.4392157306507114\n",
      "-1.6563879860222424\n",
      "0.0\n",
      "-0.2823677210245171\n",
      "-5.5494479692483365\n",
      "0.0\n",
      "-3.6012943966911757\n",
      "0.0\n",
      "matrix max difference with n = 2, max_diff = 2.420164424822787\n",
      "max difference with n = 0.25, max_diff = 2.420164424822787\n",
      "max difference with n = 0.5, max_diff = 2.420164424822787\n",
      "max difference with n = 1, max_diff = 2.420164424822787\n",
      "max difference with n = 2, max_diff = 2.420164424822787\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "mlp = NN(hidden_dims = (512, 512), weight_initer = NormalInitializer(), input_size = 784, output_size = 10)\n",
    "ha_1, hs_1, ha_2, hs_2, oa, os = mlp.forward(X_train[0])\n",
    "\n",
    "# grab the true gradients for the weights of the first layer\n",
    "true_gradients = mlp.backward(y_train[0], X_train[0], ha_1, hs_1, ha_2, hs_2, oa, os)\n",
    "\n",
    "# calculate the finite difference for different values of epsilon\n",
    "# N = [i for i in range(1,10)]\n",
    "N = [0.25, 0.5, 1, 2]\n",
    "limit = 10\n",
    "for n in N:\n",
    "    finite_diff = mlp.calc_finite_diff(y_train[0], X_train[0], eps = math.pow(10, -n), limit = limit)\n",
    "    difference = np.abs(true_gradients[3][:limit] - finite_diff[3][:limit])\n",
    "#     print(true_gradients[3][:limit].shape)\n",
    "    max_diff = np.max(difference)\n",
    "    print(f\"matrix max difference with n = {n}, max_diff = {max_diff}\")\n",
    "    \n",
    "for n in N:\n",
    "    ha_1, hs_1, ha_2, hs_2, oa, os = mlp.loop_fprop(X_train[0])\n",
    "#     print('gradients computed by bprop ', mlp.loop_bprop(y_train[0], X_train[0], ha_1, hs_1, ha_2, hs_2, oa, os))\n",
    "    finite_grads = mlp.loop_finite_diff(y_train[0], X_train[0], eps = math.pow(10, -n), limit = limit)\n",
    "    diff = np.abs(true_gradients[3][:limit] - finite_grads[3][:limit])\n",
    "    max_diff = np.max(diff)\n",
    "#     print('gradients finite differences', )\n",
    "    print(f\"max difference with n = {n}, max_diff = {max_diff}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute train/valid/test loss and accuracy and display training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 shape  (512, 784)\n",
      "b1 shape  (512,)\n",
      "W2 shape  (512, 512)\n",
      "b2 shape  (512,)\n",
      "W3 shape  (10, 512)\n",
      "b3 shape  (10,)\n",
      "W1 shape  float64\n",
      "b1 shape  float64\n",
      "W2 shape  float64\n",
      "b2 shape  float64\n",
      "W3 shape  float64\n",
      "b3 shape  float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epochs')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAucAAAERCAYAAADYJAlRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzs3Xl8FfW9//HXJycLCWEJuwQxKCiLIGJcem2timxaRSsKuFRbWnqt2t6ft63W22pr661eW1vbuhQFa90QsVZaUUDFakWBgIqsisgSkH0NEEKSz++PmYRDOAlBcnJOkvfz8ZjHmflu5zPUx/STOd/5jrk7IiIiIiKSeCmJDkBERERERAJKzkVEREREkoSScxERERGRJKHkXEREREQkSSg5FxERERFJEkrORURERESShJLzGpjZfWa21MwWmNmLZtY6RptmZjbHzD40s0Vm9ououm5mNtvMlpvZc2aWHpafY2bzzazUzEbU5zmJiIiISPJSch4ys3PN7C9VimcAJ7t7P+Bj4Ccxuu4Dznf3U4D+wFAzOyusuxf4nbt3B7YBY8Ly1cD1wDN1ehIiIiIi0qApOa+Bu09399Lw8D2gS4w27u5F4WFauLmZGXA+MDmsewK4NOyz0t0XAOXxjF9EREREGhYl57X3LeCVWBVmFjGzD4CNwAx3nw20BbZHJfeFQG69RCoiIiIiDVJqogNINDObDWQA2UCbMMkGuNXdp4Vt/gcoBZ6ONYa7lwH9wznpL5rZycD6uAcvIiIiIo1Kk0/O3f1MCOacA9e7+/XR9WZ2PfA1YKC7+2HG2m5mM4GhwG+B1maWGt497wKsrfMTEBEREZFGQ9NaamBmQ4EfA5e4+55q2rSvWMXFzDKBQcDSMJGfCVSsxnId8FL8oxYRERGRhkrJec3+BLQAZpjZB2b2CICZdTazqWGbY4CZZrYAmEsw5/yfYd2twC1mtpxgDvr4sP/pZlYIXAH82cwW1d8piYiIiEiyssPM1BARERERkXqiO+ciIiIiIkmiST8Q2q5dO8/Ly0t0GCIiR2zevHmb3b19ouOoT7pmi0hDdSTX7CadnOfl5VFQUJDoMEREjpiZrUp0DPVN12wRaaiO5JqtaS0iIiIiIklCybmIiIiISJKIa3JuZkPNbJmZLTez22LUZ5jZc2H9bDPLi6r7SVi+zMyGRJVPMLONZrawylhtzGyGmX0SfubE89xEREREROpa3JJzM4sADwLDgN7AaDPrXaXZGGCbu3cHfgfcG/btDYwC+hC8bfOhcDyAv4RlVd0GvO7uPYDXw2MRERERkQYjnnfOzwCWu/sKdy8BJgLDq7QZDjwR7k8GBpqZheUT3X2fu38GLA/Hw93fArbG+L7osZ4ALq3LkxERERERibd4Jue5wJqo48KwLGYbdy8FdhC8SbM2favq6O6fh/vrgY6xGpnZWDMrMLOCTZs21eY8RERERETqRaN8INSD157GfPWpu49z93x3z2/fvkktESwiIiIiSS6e65yvBY6NOu4SlsVqU2hmqUArYEst+1a1wcyOcffPzewYYOPRBB9TeRnMuAPO/C607lrnw4uISN36xT8WsXjdzkSHISKNSO/OLbnz4j5xGz+ed87nAj3MrJuZpRM84DmlSpspwHXh/gjgjfCu9xRgVLiaSzegBzDnMN8XPdZ1wEt1cA4H27oC5j8J486DVe/W+fAiIiIi0rTF7c65u5ea2U3ANCACTHD3RWZ2F1Dg7lOA8cCTZrac4CHPUWHfRWY2CVgMlAI3unsZgJk9C5wLtDOzQuBOdx8P3ANMMrMxwCrgyjo/qXY94Duvw7Oj4ImL4Wv3w4Bv1PnXiEgD4A5l+6F8f/hZFrUfHlfWl4ZlpWFZ1Gf5fjjpQkjLTPQZNUrNOv6TrLSliQ5DRBqRZm16EiwoGB/xnNaCu08FplYpuyNqvxi4opq+dwN3xygfXU37LcDAo4m3Vtr1gG+/BpO/BVNuho1LYNAvIRLXf0qRpq28DEqLoXRf+Bm9f7jPWraNmUiXRiXcpVFJ9X7w8ro7v1uWKDkXEREgzsl5o5WZA1c9D9N/Cu89BJuWwYgJkNk60ZGJJFZ5GZQUwb6i8HNXsFXuF0HJrqj9Km32742dPJeXHmVgBqnNIDWj+s+0TEhJg0gapKQGW8V+ZVla8Id4SkV5ajV9KsoiNdSnHhizuR5Oj5dbz7g10SGIiBwRJedfVCQVht0DHXrBy/8Nj10AoydCu+6JjkzkyJWXQ/F22LMF9mwNE+iiqMS6CPbtjNqPSrijE+/9u2v3fSmpkNEC0ltARnaw36w1tDgmTJYPk0jH/KyhLpIGZvH9NxQREakDSs6P1mnXQdvuMOlaeOx8uOIvcML5iY5Kmjr3IGHevQl2b4Y9m8P9TbB7S/C5Z3NQt3tTkJQf7u50JCNIojOyw6S6BWR3gPTjw/IWkJ4d1Sa7Snk2ZLQM9lMzlCyLiIjEoOS8LuSdDd+ZCc+OhqdGwNBfwxljlXxI3SrZfXByfVCCHZV87wnry0pij5PeApq3C6ZStO4KuQOC/aywLDMndoIdSavf8xUREWmClJzXlZzjYMw0+Nt34ZUfw4ZFcOFvIDU90ZFJsisvh6L1sG0lbFsVfO5Yc+Cud0XiXbo3dv/UTMgOk+sWx0CnftC87cEJd/N2wZbVDtKa1efZiYiIyBFQcl6XMlrAyKdg5q/g7d/CluVw5ZNBoiRNW/FO2L7q4AR828qwbBWU7YtqbNCiUzBlJKsdtDvxQHLdvH1U0h1u6c0Tc04iIiJS55Sc17WUFBh4B7TvBS/dCI+eGzwo2jF+62FKEigrhZ2FB5Luqkn43q0Ht89oFfza0r4nnDgEcvKCrXUetD42mJMtIiIiTY6S83jpdwW0OR4mXgXjB8PXH4WeFyY6Kvmi3INVTLathO0rD03AdxRC8J6sQEpqMJ+79XHQe3iYfB93IAnPzKn/cxAREZGkp+Q8nrqcBmNnBgn6xKtg4M/gy7foQdFkVrYfNn8M6z+CDQth62cHkvCSXQe3bd4+SLSPPQP6XnEg8c45DlrmBmtci4iIiBwBJefx1rIzfPOVYIrL63fBxqVwyR/1UF4yKN4B6xcGifj6j2DDR8EbXytWOYlkQJtuQcKd9+Vw2kl497t112A1ExEREZE6pOS8PqRlwuXjoUNveOOXsPVTGPVM8NCfxJ97MO2kIglfvyD43L7qQJusdnBMPzjrhmC1k059oc0JwcumRJo4MxsKPABEgMfc/Z4q9ecAvwf6AaPcfXJU3avAWcC/3f1rUeXjgXzAgI+B6929KN7nIiKS7JR51BczOOeHwQOAfxsL486DUU8Ha0xL3Sktgc3LohLxMBkv3hE2sOClUbmnwWnXB0l4p76Q3VHTjURiMLMI8CAwCCgE5prZFHdfHNVsNXA98MMYQ9wHZAHfrVL+/9x9Z/gd9wM3AfcgItLEKTmvb72+BmOmBy8senwYXPoQnHx5oqNqmPZuizEtZSmU7w/q07KCVXL6fD1MwvtBx95aelDkyJwBLHf3FQBmNhEYDlQm5+6+Mqwrr9rZ3V83s3NjlFck5gZkAh6H2EVEGhwl54nQ6WT4zhsw6VqY/K1gnvO5twfLMMqh3IMpKNGJ+PqPYMfqA22yOwYJePcLDiTibY7XQ5kiRy8XWBN1XAicWRcDm9njwIUEif5/V9NmLDAWoGvXrnXxtSIiSU3JeaJkt4dvTIGXb4G37gsS9Mv+rIcM3YO3Y66dD2vnBZ/rP4J94bQUS4G2PYIVUk4fEzUtpUNi4xaRI+bu3wynzfwRGAk8HqPNOGAcQH5+vu6ui0ijp+Q8kVLTg5VbOvaBabfDhCEw+tlgJZCmYvcWWDf/QDK+bn7wqnqASHpwB7zviOBhzY59oUMvSM9KbMwiTcta4Nio4y5hWZ1w97JwqsyPiZGci4g0NUrOE80sWCGkXQ94/lvBg6Ijn4LjvpToyOpeyW74fEF4RzxMxLetDCsteFi2x+DgIdnc06BDn+APGBFJpLlADzPrRpCUjwKuOpoBw3nmJ7j78nD/EmDpUUcqItIIKDlPFt0vgO+8Ds+MhCcuhq/9DgZcm+iovriy/cFUncpE/H3YuBg8fF6sVVfIPRXyvwWdB0Dn/pDRIrExi8gh3L3UzG4CphEspTjB3ReZ2V1AgbtPMbPTgReBHOBiM/uFu/cBMLO3gZ5AtpkVAmOAGcATZtaSYCnFD4Eb6v3kRESSkJLzZNKuR5CgP389TLkpSG4H3ZX8a227w9YVB09N+fxDKC0O6jNzgjvhPS8KEvHcAZojLtKAuPtUYGqVsjui9ucSTHeJ1fcr1Qx7dp0FKCLSiCR51tcEZebA1S/A9P+B9x6ETUthxATIbJ3oyA7Ytf7gRHztfCjeHtSlZgZ3wfPHHJiekpOnNcRFREREakHJeTKKpMKwe4OHH1/+b3jsArjqOWh7Qny+r2x/MB98/x7Yvzfc3wv7w8+SPcEKKhWJ+M7wWTCLBOuG9x4eJOG5pwXzxpP9Tr+IiIhIklIWlcxOuz5YNnDStfDoecFSi21OCJPocCvZcyCRjt4/KMnec/B+1X7lpbWLp83x0PVLBxLxTn21coqIiIhIHVJynuzyzg5eWPTsaHh2VO36pKQFb8dMz4K0TEhrHnymZ0Hz9gf206K29Kr70f2aQ/N2wZQbEREREYkbJecNQU4ejJkOS6cGL+E5JJGu2MJEOpKW6IhFRERE5AtQct5QZLSAU0YmOgoRERERiaOURAcgIiIiIiIBJeciIiIiIklCybmIiIiISJJQci4iIiIikiSUnIuIiIiIJAkl5yIiIiIiSSKuybmZDTWzZWa23Mxui1GfYWbPhfWzzSwvqu4nYfkyMxtyuDHNbKCZzTezD8zs32bWPZ7nJiIiIiJS1+KWnJtZBHgQGAb0BkabWe8qzcYA29y9O/A74N6wb29gFNAHGAo8ZGaRw4z5MHC1u/cHngF+Gq9zExERERGJh3jeOT8DWO7uK9y9BJgIDK/SZjjwRLg/GRhoZhaWT3T3fe7+GbA8HK+mMR1oGe63AtbF6bxEREREROIinm8IzQXWRB0XAmdW18bdS81sB9A2LH+vSt/ccL+6Mb8NTDWzvcBO4KxYQZnZWGAsQNeuXY/sjERERERE4qgxPRD6/4AL3b0L8Dhwf6xG7j7O3fPdPb99+/b1GqCIiIiISE3imZyvBY6NOu4SlsVsY2apBNNRttTQN2a5mbUHTnH32WH5c8B/1M1piIiIiIjUj3gm53OBHmbWzczSCR7wnFKlzRTgunB/BPCGu3tYPipczaUb0AOYU8OY24BWZnZiONYgYEkcz01EpMmoxcpb54SrZZWa2Ygqda+a2XYz+2eV8qfDMRea2QQzS4v3eYiINARxS87dvRS4CZhGkChPcvdFZnaXmV0SNhsPtDWz5cAtwG1h30XAJGAx8Cpwo7uXVTdmWP4d4AUz+xC4FvhRvM5NRKSpqOXKW6uB6wlWyqrqPoJrclVPAz2BvkAmwXNDIiJNXjwfCMXdpwJTq5TdEbVfDFxRTd+7gbtrM2ZY/iLw4lGGLCIiB6tcJQvAzCpWyVpc0cDdV4Z15VU7u/vrZnZujPLK67iZzSGYpigi0uQ1pgdCRUSk7sVaeSu3mrZHLJzOci3Br6Sx6seaWYGZFWzatKmuvlZEJGkpORcRkUR6CHjL3d+OVakVtkSkqYnrtBYREWnwarPy1hdiZncC7YHv1sV4IiKNge6ci4hITWqz8tYRM7NvA0OA0e5+yFx1EZGmSsm5iIhUqzYrb5nZ6WZWSPCA/5/NbFFFfzN7G3geGGhmhWY2JKx6BOgIvGtmH5jZHYiIiKa1iIhIzWqx8tZcqlltxd2/Uk25/v9HRCQG3TkXEREREUkSSs5FRERERJKEknMRERERkSSh5FxEREREJEkoORcRERERSRJKzkVEREREkoSScxERERGRJKHkXEREREQkSSg5FxERERFJEkrORURERESShJJzEREREZEkoeRcRERERCRJKDkXEREREUkSSs5FRERERJKEknMRERERkSSh5FxEREREJEkoORcRERERSRJKzkVEREREkoSScxERERGRJKHkXEREamRmQ81smZktN7PbYtSfY2bzzazUzEZUqXvVzLab2T+rlN8Ujudm1i7e5yAi0lAoORcRkWqZWQR4EBgG9AZGm1nvKs1WA9cDz8QY4j7g2hjl7wAXAKvqLFgRkUZAybmIiNTkDGC5u69w9xJgIjA8uoG7r3T3BUB51c7u/jqwK0b5++6+Mj4hi4g0XErORUSkJrnAmqjjwrCsXpjZWDMrMLOCTZs21dfXiogkjJJzERFJWu4+zt3z3T2/ffv2iQ5HRCTulJyLiEhN1gLHRh13CctERCQO4pqc1+IJ/wwzey6sn21meVF1PwnLl5nZkMONaYG7zexjM1tiZt+P57mJiDQRc4EeZtbNzNKBUcCUBMckItJopcZr4Kgn/AcRzFGca2ZT3H1xVLMxwDZ3725mo4B7gZHhSgCjgD5AZ+A1Mzsx7FPdmNcT3N3p6e7lZtYhXucmIom1f/9+CgsLKS4uTnQocdesWTO6dOlCWlpaQr7f3UvN7CZgGhABJrj7IjO7Cyhw9ylmdjrwIpADXGxmv3D3PgBm9jbQE8g2s0JgjLtPC2+g/BjoBCwws6nu/u0EnKKIxJmu2Ucmbsk5UU/4A5hZxRP+0cn5cODn4f5k4E9mZmH5RHffB3xmZsvD8ahhzBuAq9y9HMDdN8bx3EQkgQoLC2nRogV5eXkEl4zGyd3ZsmULhYWFdOvWLZFxTAWmVim7I2p/LsF0l1h9v1JN+R+AP9RhmCKSpHTNPjLxnNZSmyf8K9u4eymwA2hbQ9+axjyB4K57gZm9YmY9YgWlJ/9FGr7i4mLatm3bqC/yAGZG27Ztm8TdJhFpvHTNPjKN6YHQDKDY3fOBR4EJsRodzZP/5eXOzGUb2bRr39FHKyJHpbFf5Cs0lfMUkcatqVzL6uI845mc1+YJ/8o2ZpYKtAK21NC3pjELgb+F+y8C/Y76DKpYvXUP33rqVZ54b1ldDy0iEndmdrOZ5SQ6DhERqV48k/PaPOE/Bbgu3B8BvOHuHpaPCldz6Qb0AOYcZsy/A+eF+18FPq7zM0rbRPYJv+GZxS+wv+yQF+GJSBOxfft2HnrooSPud+GFF7J9+/Y4RFRrHQkepJ8UrnzVNG5liUiT15Cu23FLzsM55BVP+C8BJlU84W9ml4TNxgNtwwc+bwFuC/suAiYRPOj5KnCju5dVN2Y41j3A5Wb2EfBroM6f+s9rlUfX7JMoznybVz76vK6HF5EGorqLfGlpaY39pk6dSuvWreMV1mG5+08JbnaMJ1jh6hMz+18zOyFhQYmI1IOGdN2O52ottXnCvxi4opq+dwN312bMsHw7cNFRhnxY3znlGn4266c8MudVLuk/Jt5fJyJJ6LbbbuPTTz+lf//+pKWl0axZM3Jycli6dCkff/wxl156KWvWrKG4uJgf/OAHjB07FoC8vDwKCgooKipi2LBhfPnLX2bWrFnk5uby0ksvkZmZGffY3d3NbD2wHiglWP5wspnNcPcfxz0AEZEEaEjX7bgm543RsOOHcvd7/8eKfTNYtG4EfTq3SnRIIk3aL/6xiMXrdtbpmL07t+TOi/tUW3/PPfewcOFCPvjgA958800uuugiFi5cWLl01oQJE2jTpg179+7l9NNP5/LLL6dt27YHjfHJJ5/w7LPP8uijj3LllVfywgsvcM0119TpeVRlZj8AvgFsBh4DfuTu+80sBfiEYN1xEZG4ScQ1GxrWdbsxrdZSLzIiGYw48XJSWyzmz/+el+hwRCQJnHHGGQetafuHP/yBU045hbPOOos1a9bwySefHNKnW7du9O/fH4DTTjuNlStX1keobYCvu/sQd3/e3fcDhO+H+Fp9BCAikgyS+bqtO+dfwLV9RvHU0r/w2tqX2Lb7y+Q0T090SCJN1uHultSH5s2bV+6/+eabvPbaa7z77rtkZWVx7rnnxlzzNiMjo3I/Eomwd+/e+gj1FWBrxYGZtQR6uftsd19SHwGISNOWDNdsSO7rtu6cfwGdszuT3/5srOVsnp27ItHhiEg9a9GiBbt27YpZt2PHDnJycsjKymLp0qW899579RxdjR4GiqKOi8IyEZFGrSFdt3Xn/Av6Tv9rKdj0b/7y4Ut895yTiKRoRTKRpqJt27acffbZnHzyyWRmZtKxY8fKuqFDh/LII4/Qq1cvTjrpJM4666wERnoIC5erBYLpLOE7JkREGrWGdN22qOt0k5Ofn+8FBQVfqG+5lzPwuYvYsC2FB746gUG9Ox6+k4jUiSVLltCrV69Eh1FvYp2vmc0L34hca2b2N+BNDtwt/x5wnrtfWhdxxtvRXLNFJHF0zT6ya7amtXxBKZbCN/teRSRrNQ/PejPR4YiI1MZ/Av9B8GblQuBMYGxCIxIRkYMoOT8Kl/W4lFRrxuLdr7B8Y+x5TCIiycLdN7r7KHfv4O4d3f0qd9+Y6LhEROSAWiXnZnaCmWWE++ea2ffNLHGvuUsSLdJbMCzvQlJbfsij7yxMdDgiIjUys2ZmdqOZPWRmEyq2RMclIiIH1PbO+QtAmZl1B8YBxwLPxC2qBuT6vldjKaX8c8VL7Cren+hwRERq8iTQCRgC/AvoAuhnPxGRJFLb5Lzc3UuBy4A/uvuPgGPiF1bDcWLOifRsfQq0fJfnC1YlOhwRkZp0d/efAbvd/QngIoJ55yIikiRqm5zvN7PRwHXAP8OytPiE1PCMOeUaUtK3Mn7+q5SXN93Vb0Qk6VX8vLfdzE4GWgEdEhiPiIhUUdvk/JvAl4C73f0zM+tG8POoAAO7DqRFahu2Rt7k38s3JzocEUky2dnZAKxbt44RI0bEbHPuuedSD8sEjjOzHOCnwBRgMXDv4TqZ2VAzW2Zmy83sthj155jZfDMrNbMRVepeNbPtZvbPKuXdzGx2OOZzZqZXLYtI0kjkdbtWybm7L3b377v7s+GFvYW7H/aC3lSkpaQxuteVpGYv48+zZic6HBFJUp07d2by5MkJ+W4zSwF2uvs2d3/L3Y8PV23582H6RYAHgWFAb2C0mfWu0mw1cD2xn0W6D7g2Rvm9wO/cvTuwDRhzRCckIlIPEnHdru1qLW+aWUszawPMBx41s/vjG1rDMqrnlRgRCra9zOotexIdjojE0W233caDDz5Yefzzn/+cX/3qVwwcOJABAwbQt29fXnrppUP6rVy5kpNPPhmAvXv3MmrUKHr16sVll13G3r174xqzu5cDP/4CXc8Alrv7CncvASYCw6uMvdLdFwDlMb73dao8dGpmBpwPVPw/3hNAg3gRkog0TA3pul3b1za3cvedZvZt4K/ufqeZLYhLRA1U+6z2fDX3fGaW/ZvH313GnV87NdEhiTQNr9wG6z+q2zE79YVh91RbPXLkSP7rv/6LG2+8EYBJkyYxbdo0vv/979OyZUs2b97MWWedxSWXXEKQhx7q4YcfJisriyVLlrBgwQIGDBhQt+cQ22tm9kPgOWB3RaG7b62hTy6wJuq44uVFR6MtsD1caKBizNyjHFNEGoIEXLOhYV23azvnPNXMjgGu5MADoVLF9X2vxiJ7mbz0H+wtKUt0OCISJ6eeeiobN25k3bp1fPjhh+Tk5NCpUyduv/12+vXrxwUXXMDatWvZsGFDtWO89dZbXHPNNQD069ePfv361UfoI4EbgbeAeeEW94nuR8PMxppZgZkVbNq0KdHhiEgD1ZCu27W9c34XMA14x93nmtnxwCdxiagBG9BhAF2aH8+q4n/z4vuFXHXmcYkOSaTxO8zdkni54oormDx5MuvXr2fkyJE8/fTTbNq0iXnz5pGWlkZeXh7FxcUJia067t7tC3RbS/BuiwpdwrKjsQVobWap4d3zasd093EE79cgPz9fy2GJNHQJumZDw7lu1/aB0OfdvZ+73xAer3D3y+MbWsNjZnyr7zVEmn3Oo3PewF3/PyLSWI0cOZKJEycyefJkrrjiCnbs2EGHDh1IS0tj5syZrFpV83sPzjnnHJ55Jnh+cuHChSxYEP+Zgmb2jVjbYbrNBXqEq6ukA6MIVnr5wjy4OM4EKpZAuA44dLKniEgdaijX7do+ENrFzF40s43h9oKZdYlLRA3cRcdfREZKcz7315nzWU3TOEWkIevTpw+7du0iNzeXY445hquvvpqCggL69u3LX//6V3r27Flj/xtuuIGioiJ69erFHXfcwWmnnVYfYZ8etX0F+DlwSU0dwjvbNxH8eroEmOTui8zsLjO7BMDMTjezQuAK4M9mtqiiv5m9DTwPDDSzQjMbElbdCtxiZssJ5qCPr7vTFBE5VEO5bltt7u6a2QyCJbIq1ja/Brja3QfFJap6kp+f7/FYn/Lud3/NxGXPcWbqb3nsmoF1Pr5IU7dkyRJ69eqV6DDqTazzNbN57p5/NOOaWWtgorsPPZpx6ku8rtkiEl+6Zh/ZNbu2D4S2d/fH3b003P4CtD+yUJuOq3uPBivj7Q0v8/mO+C6PJiJyFHYDX2QeuoiIxEltk/MtZnaNmUXC7RqCB3okhrxWeQxofyaprWbz5LufJTocEREAzOwfZjYl3P4JLANeTHRcIiJyQG1Xa/kW8Efgd4ADswjeBifV+Gbfa5i/6WaeXTiVH1xwEhmpkUSHJCLym6j9UmCVuxcmKhgRETlUbVdrWeXul7h7+/B1z5cCWq2lBl/J/QptMjpSnPU2Ly/4PNHhiIgArAZmu/u/3P0dgl9F8xIbkoiIRKvttJZYbqmzKBqhSEqEa3uPJrX5Ch6d/W6iwxERgWDVlPKo47KwTEREksTRJOex320qlS4/8etELI0VxdP5YM32RIcjIpLq7iUVB+F+egLjERGRKo4mOdcbdg4jp1kOQ44bSlrr95nwzpJEhyMidWT79u089NBDX6jv73//e/bs2VPHEdXapoq1yQHMbDiwOVHBiIjUl4Z03a4xOTezXWa2M8a2C+hcTzE2aNf2uQpL2cf0NS+zuWhfosMRkTrQkC7yVfwncLuZrTaz1QQvAvpuooIREakvDem6XeNqLe7eor4ykH7XAAAgAElEQVQCaaxObncyPVr1Zum+WTw7exU3Dzwx0SGJyFG67bbb+PTTT+nfvz+DBg2iQ4cOTJo0iX379nHZZZfxi1/8gt27d3PllVdSWFhIWVkZP/vZz9iwYQPr1q3jvPPOo127dsycObNe43b3T4GzzCw7PC6q1wBERBKkIV23a7uUohyF6/tezf/8+3/46wev85/ndictcjSziUQk2r1z7mXp1qV1OmbPNj259Yxbq62/5557WLhwIR988AHTp09n8uTJzJkzB3fnkksu4a233mLTpk107tyZl19+GYAdO3bQqlUr7r//fmbOnEm7du3qNObaMLP/Bf7P3beHxznAf7v7T+s9GBFpkhJxzYaGdd2Oa5ZoZkPNbJmZLTez22LUZ5jZc2H97OglvczsJ2H5MjMbcgRj/sHMkupu0JC8ITRPbcWu9H8xfdGGRIcjInVo+vTpTJ8+nVNPPZUBAwawdOlSPvnkE/r27cuMGTO49dZbefvtt2nVqlWiQwUYVpGYA7j7NuDCBMYjIlLvkv26Hbc752YWAR4EBgGFwFwzm+Lui6OajQG2uXt3MxsF3AuMNLPewCigD8Hc9tfMrGI+SLVjmlk+kBOvc/qiMiIZjDxpBBMWTuCxd+dxUb+vJTokkUbjcHdL4s3d+clPfsJ3v3vo1O358+czdepUfvrTnzJw4EDuuOOOBER4kIiZZbj7PgAzywQyEhyTiDQhib5mQ/Jft+N55/wMYLm7rwiX65oIDK/SZjjwRLg/GRhoZhaWT3T3fe7+GbA8HK/aMcM/Bu4DfhzHc/rCRva8EjNjUdF0lny+M9HhiMhRaNGiBbt27QJgyJAhTJgwgaKi4Ae7tWvXsnHjRtatW0dWVhbXXHMNP/rRj5g/f/4hfRPgaeB1MxtjZt8GZnDgGiwi0mg1pOt2POec5wJroo4LgTOra+PupWa2A2gblr9XpW9uuF/dmDcBU9z98yC/j83MxgJjAbp27XoEp3N0Omd35uzOX+Ht0jk8PusT/u/y0+rtu0WkbrVt25azzz6bk08+mWHDhnHVVVfxpS99CYDs7Gyeeuopli9fzo9+9CNSUlJIS0vj4YcfBmDs2LEMHTqUzp07J+KB0HvN7EPgAoLlcKcBx9VrECIiCdCQrtuN4oFQM+sMXAGce7i27j4OGAeQn59fr2u1X9vnKv697l/8Y/mr3L6nL62z9O4PkYbqmWeeOej4Bz/4wUHHJ5xwAkOGDKGqm2++mZtvvjmusR3GBoLE/ArgM+CFRAYjIlJfGsp1O57TWtYCx0YddwnLYrYxs1SgFbClhr7VlZ8KdAeWm9lKIMvMltfVidSVs445i2OyjsVazuL5gsJEhyMiTYSZnWhmd5rZUuCPwGrA3P08d/9TgsMTEZEo8UzO5wI9zKybmaUTPOA5pUqbKcB14f4I4A1397B8VLiaSzegBzCnujHd/WV37+Tuee6eB+xx9+5xPLcvJMVS+Eafq4hkrWZCwduUleslqyJSL5YC5wNfc/cvu/sfgbIExyQiIjHELTl391KCeeDTgCXAJHdfZGZ3Rb0+ejzQNrzLfQtwW9h3ETAJWAy8Ctzo7mXVjRmvc4iHS7pfQnpKM7ZFZvLmso2JDkekwQr+jm/86ug8vw58Dsw0s0fNbCBQ/cM5IiJ1TNfs2ovrnHN3nwpMrVJ2R9R+McG8x1h97wburs2YMdpkf5F460PL9JZcfMLXeKHsJR6btZCBvTomOiSRBqdZs2Zs2bKFtm3bUtMD4A2du7NlyxaaNWt2tOP8Hfi7mTUnWOHqv4AOZvYw8KK7Tz/6aEVEYtM1+8g0igdCG5qreo3mhU8mU7BlOp9uOpMT2ift3xIiSalLly4UFhayadOmRIcSd82aNaNLly51Mpa77waeAZ4J3w56BXArUGNybmZDgQeACPCYu99Tpf4c4PdAP2CUu0+OqrsOqHgD6a/c/YmwfCTwP+GY/3T3xC9+LCJxoWv2kVFyngAn5pxIv3YD+KDkPf466zN+MbxvokMSaVDS0tLo1q1bosNo0MK3g1auXlWdWr5QbjVwPfDDKn3bAHcC+QQrxMwzsykEUyrvA05z901m9oSZDXT31+vk5EQkqeiafWTi+UCo1ODaPqNJSd/KC0teo2hfaaLDERGpzmFfKOfuK919AVBepe8QYIa7bw3/GJgBDAWOBz5x94rbaK8Bl8fzJEREGgol5wkysOtAWqe3pazFO/xtvpZVFJGkFeuFcrnVtK1t3+XASWaWFy6jeykHL5NbyczGmlmBmRU0hZ/ERUSUnCdIWkoaV/UaSWr2Msa/N7fJPMUsIhLeRb8BeA54G1hJNUs7uvs4d8939/z27dvXX5AiIgmi5DyBRpw4ghQirPc3eGf5lkSHIyISS21eKHfEfd39H+5+prt/CVgGfFwHsYqINHhKzhOofVZ7Bna9gPSceYyftSzR4YiIxFKbF8pVZxow2MxywtVhBodlmFmH8DMH+B7wWJ1HLiLSACk5T7Cre4+GlL38+/PprNm6J9HhiIgcpDYvlDOz082skGBpxj+b2aKw71bglwQJ/lzgrrAM4AEzWwy8A9zj7rpzLiKCllJMuAEdBtCtZXc+LX6PJ99dye0X9U50SCIiB6nFC+XmEkxZidV3AjAhRvnoOg5TRKRR0J3zBDMzru1zFSnN1jHxo7fZWxLzmSgRERERaQKUnCeBi7pdRGakOSVZbzPlw9o+ZyUiIiIijY2S8ySQlZbF10+8lLSWC5nw7gItqygiIiLSRCk5TxKjThoFVsZnJW9QsGpbosMRERERkQRQcp4k8lrlcWanL5GRM4e/vPNposMRERERkQRQcp5Eru41GlJ3MGPVG2zYWZzocERERESknik5TyLndDmHDpmdiLSexdPvrUp0OCIiIiJSz5ScJ5FISoSreo0itfkKnpo/h32lWlZRREREpClRcp5kvt7j66RaGrsz3uKVj9YnOhwRERERqUdKzpNMTrMcLuw2jIzW7/P4u0sSHY6IiIiI1CMl50lodK/ReMo+Fu+ayYLC7YkOR0RERETqiZLzJHRyu5Pp3eZkMtq8x1/eWZnocERERESknig5T1JX9x6NpW/k5eVvsaVoX6LDEREREZF6oOQ8SQ3JG0LLtNZYy1lMnLsm0eGIiIiISD1Qcp6kMiIZXHHS5aS1WMyTcz+gtKw80SGJiIiISJwpOU9iV550JWbG1shbvLZkQ6LDEREREZE4U3KexDpnd+arXc6hWc5cJsz6JNHhiIiIiEicKTlPcqN7jcYjRczf/BZL1+9MdDgiIiIiEkdKzpPcWcecxbHZx5HR5l3++u6qRIcjIiIiInGk5DzJpVgKV/UaRUrmal5cNJsVm4oSHZKINDFmNtTMlpnZcjO7LUb9OWY238xKzWxElbrrzOyTcLsuqny0mX1kZgvM7FUza1cf5yIikuyUnDcAl3S/hGaRTNJz3uWyh2Yxe8WWRIckIk2EmUWAB4FhQG9gtJn1rtJsNXA98EyVvm2AO4EzgTOAO80sx8xSgQeA89y9H7AAuCme5yEi0lAoOW8AWqa35OITvkZqyw9p3Woz14yfzd/mFyY6LBFpGs4Alrv7CncvASYCw6MbuPtKd18AVF3zdQgww923uvs2YAYwFLBwa25mBrQE1sX5PEREGoS4Jue1+Ck0w8yeC+tnm1leVN1PwvJlZjbkcGOa2dNh+UIzm2BmafE8t/r2rZO/RauMlpR0+CM98zZyy6QP+d2Mj3H3RIcmIo1bLhD9JrTCsOwL93X3/cANwEcESXlvYHysAcxsrJkVmFnBpk2bjjR2EZEGJ27JeS1/Ch0DbHP37sDvgHvDvr2BUUAfgrssD5lZ5DBjPg30BPoCmcC343VuidClRReevvBpOjXvRGGzB/iPU1bxwOufcMukD9lXWpbo8EREai28eXIDcCrQmWBay09itXX3ce6e7+757du3r8coRUQSI553zg/7U2h4/ES4PxkYGP7EORyY6O773P0zYHk4XrVjuvtUDwFzgC5xPLeEOCb7GJ4Y9gSndjiVj0oe5vyzFvDi+4Vc+9gctu0uSXR4ItI4rQWOjTruEpYdTd/+AO7+aXjNngT8x9GHKiLS8MUzOa/NT6GVbdy9FNgBtK2h72HHDO/IXAu8Giuohv4Tacv0ljxywSNcdPxFzN3xDOed/RYfrNnC1x+excrNuxMdnog0PnOBHmbWzczSCX7VnFLLvtOAweFDoDnA4LBsLdDbzCpuhQ8CltRx3CIiDVJjfCD0IeAtd387VmVj+Ik0PZLOr7/8a77T9zsUbH2F0896iW17d3HZQ+8wd+XWRIcnIo1IeOPkJoKkegkwyd0XmdldZnYJgJmdbmaFwBXAn81sUdh3K/BLggR/LnBX+HDoOuAXwFtmtoDgTvr/1ve5iYgko9Q4jl2bn0Ir2hSGS2u1ArYcpm+1Y5rZnUB74Lt1EH9SMzO+P+D7HJN9DHe/dzfd+m5n26fXcvWjs7nvin4M71/b57VERGrm7lOBqVXK7ojan0s1UwndfQIwIUb5I8AjdRupiEjDF88757X5KXQKUPFSihHAG+H8wynAqHA1l25AD4J55NWOaWbfJli2a7S7V13Oq9G64sQr+MP5f2Dt7lWkdf0TvY7byw8mfsAfX/9EK7mIiIiINDBxS85r81MowdJZbc1sOXALcFvYdxHBA0KLCeaO3+juZdWNGY71CNAReNfMPjCzyrs6jd05Xc7h8SGPs7+8hM0tfst5p+zitzM+5ofPL6CktMn8nSIiIiLS4FlTvruan5/vBQUFiQ6jzhTuKuR7r3+Pwl2FfLnVzUyZ1YGzjm/DI9ecRuus9ESHJyJ1yMzmuXt+ouOoT43tmi0iTceRXLMb4wOhTVaXFl14ctiT9G3Xl5nb7ufy8z9h/qptfP3hWazaopVcRERERJKdkvNGplVGK8YNHseQvCFM/3w8Q7/6Llt2F3PZQ7OYt0oruYiIiIgkMyXnjVBGJIP/O+f/+GafbzLz87+Tf/oUWjQrZ/Sjs/nHh+sSHZ6IiIiIVCOeSylKAqVYCrfk38Ix2cdwz5x7OOmkrbRZdz03P/s+q7fu4XvnnkDwMlYRERERSRa6c97Ije45mt+d+zs+2/kpe9s/wKB+xn3TlvHjyVrJRURERCTZKDlvAs7vej7jh4xnb+kelqb8mlFfKeP5eYVc//gcduzdn+jwRERERCSk5LyJ6Ne+H08Ne4pWGa14bdtdjBm8i7krt3L5w7NYs3VPosMTEREREZScNynHtjyWJ4c9Sc82PXl+zf9y3dDVbNq1j0sffIf5q7clOjwRERGRJk/JeROT0yyHxwY/xsCuA3luxYNceO5ssjJSGD3uPaZ+9HmiwxMRERFp0pScN0HNUpvxm6/+hmt6XcM/V06i36kv0Ts3k+89PZ9H/vUpTfmtsSIiIiKJpKUUm6hISoRbz7iVztmduW/uffTtupUhLb/DPa8sZeXm3fzy0pNJi+hvNxEREZH6pOyribu297X89tzfsmzbUgoz7+MbX8lm4tw1fPPxuews1kouIiIiIvVJybkw6LhBPDb4MXaW7OBfRXfwgwvTeW/FFi5/SCu5iIiIiNQnJecCQP8O/Xly2JNkpWbxzOrbuWV4CRt2FnPZQ7P4YM32RIcnIiIi0iQoOZdKea3yeOrCp+iR04NHlt7Bty/6nMz0FEaNe5fnC9ZQvL8s0SGKiIiINGpKzuUgbTPbMn7IeL567Fd5dPFvGfKVAnodk82PJi9gwC9ncMNT8/j7+2v1ZlGRJsTMhprZMjNbbma3xag/x8zmm1mpmY2oUnedmX0SbteFZS3M7IOobbOZ/b6+zkdEJJlptRY5RGZqJr8/9/fcM+ceJi57ksG9N3LT+f+PmUu3Mn3RBl5ZuJ7UFONLJ7RlcJ9ODO7dkY4tmyU6bBGJAzOLAA8Cg4BCYK6ZTXH3xVHNVgPXAz+s0rcNcCeQDzgwL+y7Degf1W4e8Ld4noeISEOh5FxiiqREuP3M28nNzuW3837LZztXMLzHcG4YOIgN2zKZtmg90xdt4Gd/X8jP/r6QU7u2ZnDvTgzp05Hj22cnOnwRqTtnAMvdfQWAmU0EhgOVybm7rwzryqv0HQLMcPetYf0MYCjwbEUDMzsR6AC8Hb9TEBFpOJScS7XMjOtPvp4uLbowbsE4flPwG35T8Bv6tuvLoOMG8dezL6B4b2umLVrPtEUbuPfVpdz76lJ6dMhmSJ9ODO7Tkb65rTCzRJ+KiHxxucCaqONC4Myj6Jtbpc0o4Dmv5u1nZjYWGAvQtWvXWn6tiEjDpeRcDuuC4y7gguMuYM3ONUxfNZ0Zq2Zw/7z7uX/e/fRq04vBeYN58LrBRMrbMyNM1B/+16f8aeZyOrdqFkx96dORM/LakKoXG4nIwUYB11ZX6e7jgHEA+fn5en2xiDR6Ss6l1o5teSxj+o5hTN8xrC1ay2urXmP6yuk8MP8BHpj/ACflnMSg4wbxvyMH0zp1AK8t2cD0xRt4ds5q/jJrJa2z0hjYsyND+nTkKz3ak5keSfQpicjhrQWOjTruEpbVtu+5Vfq+WXFgZqcAqe4+7+hCFBFpPKyaXxKbhPz8fC8oKEh0GA3e50Wf89rq15ixagbvb3wfgO6tuzP4uMEMOm4QnZvn8dbHm5i2aAOvL9nAzuJSMtMinHNiO4b06cTAnh1plZWW4LMQaVjMbJ6759fD96QCHwMDCZLtucBV7r4oRtu/AP9098nhcRtgHjAgbDIfOC1qDvo9wD53v7M2seiaLSIN1ZFcs5Wc60Jfpzbs3lCZqM/fMB/HOb7V8Qw6bhCDjhtEt5bdmfPZtuCB0sXr2bBzH6kpxlnHt2Vwn44M7t2JTq208ovI4dRXch5+14XA74EIMMHd7zazu4ACd59iZqcDLwI5QDGw3t37hH2/BdweDnW3uz8eNe4K4EJ3X1qbOHTNFpGGSsl5LelCH1+b927mtVVBol6woYByL+e4lsdVJuonte7JgrU7mL54A9MWrWfFpt0AnHJsawb37siQPp3o3kErv4jEUp/JebLQNVtEGiol57WkC3392bJ3C2+seYPpK6czd/1cyryMLtldGJQ3iMHHDaZP2z58uqmIaYuCRH1B4Q4ATmjfnNOOy6Fz60w6t86kS/jZqVUzmqVpzro0XUrORUQaDiXntaQLfWJsK97GzDUzmb5qOrPXzabUS+ncvHNwRz1vEH3b9WX9jn3MWLyBGYs38PGGXWzcte+Qcdq3yKBz60xyWzejc6tMcnMyw+PgMycrTcs4SqOl5FxEpOFQcl5LutAn3o59O5i5ZiYzVs1g1rpZlJaX0jGrY+XUl/4d+pNiKewrLWPDjn2s3b6Xtdv3si7c1kZ9Fu8/+P0nmWkROrduVpmwVyTtFcedWjUjPVVLO0rDpORcRKThUHJeS7rQJ5edJTv515p/MX3VdGatnUVJeQntM9vTv0N/umR3ITc7l9wWueRm59I5uzMZkYzKvu7Otj37KxP1tdvCBH7HXtZuL2bttr1sLjr47rsZdAjvvkdPmQm24G58y8w0Iim6+y7JR8m5iEjDcSTXbK1zLkmjZXpLLj7hYi4+4WKKSop4q/AtXlv9Gh9v+5g317zJ/vL9B7Vvn9n+oIS9YuvTNZeBvY4lNeXg/7yL95exfkfxgQS+8g58MUvW7eS1xRvYV1r17eOQkZpC84xUstIj4ZZK84wImWnBZ1Z6UNc8PUJWZbtUmqdHyEyPRPVNrWyTmRZR0i8iIiKHUHIuSSk7PZsLj7+QC4+/EIByL2fjno2sK1rH2qK1FBYVsnbXWtbtXsf7G97nlc9eodwPJNYRi9Axq+MhiXtudi4nHJPLWSfkkmIHT2lxd7bsLqmcMrNuezFF+0rZXVLKnn1l7C4pZW9JGbtLytizr5Rtu/eyp6SU3SVlYXkpR/JDVLO0FJqnpwYJfHoqWRkHkv/MtAipESMtJYXUiJGaYqRGovZTUkiLGJHws7I+/AzqotsZaZX1QXn0Z/T3RFKMFAs2SyHcDz7NqKxLMTSnX0REpI4pOZcGIcVS6NS8E52ad2JAxwGH1O8v38/63esPJO+7CllbtJZ1Ret4Z+07bNq76aD2aSlpdM7ufEjinpudS5d2ufTN7XTEiae7U7y/PCqJL2X3vgOJ+56SUvaUlFVJ9A8k/ntKythTUsaWoj3s3V9GaZlTWl4efjqlZeXsL3fKwi1ZpEQl7GZUJvcHEvmKxN6i2gaJfUrKwX0r/sXNDIOwLKiLLqeiLqrewsKD6mOMVdGg6viV38GB/92r/idQ8d+EHVTGQWU1takordrn3sv7kdM8vYZ/ZRERaSrimpyb2VDgAYIXVzzm7vdUqc8A/gqcBmwBRrr7yrDuJ8AYoAz4vrtPq2lMM+sGTATaEryR7lp3L4nn+UnySEtJ49gWx3Jsi2Nj1heXFrNu97oged+1tvLu+7qidSzespjt+7Yf1D4zNZPc7FxaprekeVpzstOyyUrLIjstm+ZpzQ9s6c1pntqc7PRsslKzyE7PpnlqczplNSctJStu51teHiTsZeXO/soEPvys2C939peVB23KguS+tDwq0S8L+peWV+wfKHN3yh3K3fHw88Dxgf1yJzyu0r78CNt7+L0VJ+jgBHUevQ/hrxNe+StFUOaVdRXHcHDfqscOeDk45QeNVRlCxRgc6Fu1DdW28apNDukf/bxPWRN+9kdERA4Wt+TczCLAg8AgoBCYa2ZT3H1xVLMxwDZ3725mo4B7gZFm1hsYBfQBOgOvmdmJYZ/qxrwX+J27TzSzR8KxH47X+UnD0iy1Gce3Op7jWx0fs76opKjyTvvaorWV+7v272Lz3s2s3rWaopIi9pTuYW/p3lp9Z3pK+kFJe3TyXpHUH5TcRyX/WalZpEXSSEuJ2qKOIxapXGkmE633LiIi0ljE8875GcByd18BYGYTgeFAdHI+HPh5uD8Z+JMFvwkPBya6+z7gMzNbHo5HrDHNbAlwPnBV2OaJcFwl51Ir2enZnNTmJE5qc9Jh25aWl7KndA+7S3aze/9uivYXsWf/Hor2F7F7/+5Dtuj6TXs2sap01REn+lUZdkjCHus4NSX1sG0OSfxTIkQsQoqlkGIpNe+npJDCgfJISs39ajNWMA3FSLED+2Z2oC66nsMca068iIg0MPFMznOBNVHHhcCZ1bVx91Iz20EwLSUXeK9K39xwP9aYbYHt7l4ao/1BzGwsMBaga9euR3ZGIkBqSiot01vSMr3lUY9VXaK/u3Q3peWl7C/fz/6y/cFnxXaY49Ly0oPKSspK2F2y++A+1fRrjKKTe4zKJL/igeDKpD6cj26V88KtsvzAPPJD6zAOaXPIcZX9quOMHzKedpnt6u8fpSl55TZY/1GioxCRxqRTXxh2z+HbfUFN7oFQdx8HjINgzdwEhyNNXF0m+kfL3Sv/ICj3csq8jHIvr9wqjg/6LC+nnPKDjqPr3T12v1jjlpeF88GdcsqDeeTuOE65l1fWVRwDleUHHYf9cSrHqdq+unEPzDM/UFbxb1O5X9t2FWNWrauY9x41TlpKWtz/9xURkYYhnsn5WiD66bwuYVmsNoVmlgq0IngwtKa+scq3AK3NLDW8ex7ru0SkBmYWTG+JKFGURiSOd7dEROIhnu8unwv0MLNuZpZO8IDnlCptpgDXhfsjgDc8uJU0BRhlZhnhKiw9gDnVjRn2mRmOQTjmS3E8NxERERGROhe3O+fhHPKbgGkEyx5OcPdFZnYXUODuU4DxwJPhA59bCZJtwnaTCB4eLQVudPcygFhjhl95KzDRzH4FvB+OLSIiIiLSYJg34fV18/PzvaCgINFhiIgcMTOb5+75iY6jPumaLSIN1ZFcs+M5rUVERERERI6AknMRERERkSSh5FxEREREJEkoORcRERH5/+3df6jddR3H8eerbdBUWJYxzFkLGob90Il/WEJ/aEFkZNAfGhUS/pOEWkRp/0dIRNhKArN04NA/lln0x1CmVJD0y9bcj0CoZautbcSsRZjZuz/Od3RZW3j1nvP5nvN5PuByvudzx9n7zT33dd/3c7/ne6SRcDiXJEmSRqLrq7UkOQr8vnUdy3AecKx1EQ302HePPUOffb/Unt9QVa9d6WLGbA4zG3xO96THvnvsGV5a3y86s7sezudNkl/0duk06LPvHnuGPvvuseee9Pj17bFn6LPvHnuG6fftaS2SJEnSSDicS5IkSSPhcD5f7m5dQCM99t1jz9Bn3z323JMev7499gx99t1jzzDlvj3nXJIkSRoJd84lSZKkkXA4lyRJkkbC4XwOJLkwyeNJ9iXZm+TW1jXNSpJVSX6V5Aeta5mVJK9Ksj3Jb5LsT/KO1jVNW5JPD8/tPUkeSPLK1jVNQ5JvJzmSZM+StVcneTTJ08PtuS1r1MtnZpvZrWuahR5yu1VmO5zPh38Bn6mqi4ErgE8mubhxTbNyK7C/dREz9lVgR1W9GbiEBe8/yQXALcDlVfVWYBVwfduqpuY+4L2nrN0O7KyqTcDO4b7mm5ndl64yG7rK7ftokNkO53Ogqg5V1ZPD8d+YfONf0Laq6UuyAbgGuKd1LbOSZB3wLuBbAFX1z6o63raqmVgNrE2yGjgL+FPjeqaiqn4E/OWU5WuBrcPxVuCDMy1KK87MNrPbVjUzC5/brTLb4XzOJNkIbAZ+2raSmbgT+Bzw79aFzNAbgaPAvcOfhu9Jcnbroqapqv4IfBl4BjgEPFtVj7StaqbWV9Wh4fgwsL5lMVpZZvbC6y6zofvcnnpmO5zPkSTnAN8BPlVVf21dzzQleT9wpKp+2bqWGVsNXAZ8o6o2A39nwU9zGM7Xu5bJD7nXAWcn+WjbqtqoybVtvb7tgjCzu9BdZoO5fdK0MtvhfE4kWcMk5LdV1UOt65mBK4EPJDkAPAhcleT+tiXNxEHgYFWd3GXbzgMY4nUAAAMQSURBVCT4F9m7gd9V1dGqeh54CHhn45pm6c9JzgcYbo80rkcrwMw2sxdcz7k99cx2OJ8DScLkfLb9VfWV1vXMQlV9vqo2VNVGJi8yeayqFv638qo6DPwhyUXD0tXAvoYlzcIzwBVJzhqe61fTwQuqlvg+cMNwfAPwvYa1aAWY2WZ2w5JmpefcnnpmO5zPhyuBjzHZidg1fLyvdVGampuBbUl2A5cCX2xcz1QNO07bgSeBp5jk0kK+JXSSB4AngIuSHExyI3AH8J4kTzPZjbqjZY1aEWZ2X7rKbOgnt1tldiany0iSJElqzZ1zSZIkaSQcziVJkqSRcDiXJEmSRsLhXJIkSRoJh3NJkiRpJBzOpdNI8sKSS6DtSrJi7/iWZGOSPSv1eJLUOzNbi2R16wKkkfpHVV3aughJ0otiZmthuHMuLUOSA0m+lOSpJD9L8qZhfWOSx5LsTrIzyeuH9fVJvpvk18PHybc3XpXkm0n2Jnkkydrh39+SZN/wOA82alOSFoKZrXnkcC6d3tpT/kR63ZLPPVtVbwO+Dtw5rH0N2FpVbwe2AVuG9S3AD6vqEuAyYO+wvgm4q6reAhwHPjSs3w5sHh7nE9NqTpIWjJmtheE7hEqnkeREVZ1zmvUDwFVV9dska4DDVfWaJMeA86vq+WH9UFWdl+QosKGqnlvyGBuBR6tq03D/NmBNVX0hyQ7gBPAw8HBVnZhyq5I098xsLRJ3zqXlqzMcL8dzS45f4L+v/7gGuIvJjs3Pk/i6EEl6ecxszRWHc2n5rlty+8Rw/BPg+uH4I8CPh+OdwE0ASVYlWXemB03yCuDCqnocuA1YB/zPTpAkaVnMbM0Vf8OTTm9tkl1L7u+oqpOX5jo3yW4mOykfHtZuBu5N8lngKPDxYf1W4O4kNzLZbbkJOHSG/3MVcP/wwyDAlqo6vmIdSdLiMrO1MDznXFqG4fzFy6vqWOtaJEn/n5mteeRpLZIkSdJIuHMuSZIkjYQ755IkSdJIOJxLkiRJI+FwLkmSJI2Ew7kkSZI0Eg7nkiRJ0kj8B4rRTmWoQIEeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp = NN(hidden_dims = (512, 512), weight_initer = ZeroInitializer(), input_size = 784, output_size = 10)\n",
    "\n",
    "train_accuracies, train_losses = [], []\n",
    "valid_accuracies, valid_losses = [], []\n",
    "test_accuracies, test_losses = [], []\n",
    "num_epochs = 10\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    loss = mlp.train(X_train, y_train_onehot, mb_size=100)\n",
    "    loss_train, pred_train = mlp.test(X_train, y_train_onehot)\n",
    "    loss_valid, pred_valid = mlp.test(X_valid, y_valid_onehot)\n",
    "    loss_test, pred_test = mlp.test(X_test, y_test_onehot)\n",
    "    \n",
    "    valid_losses.append(loss_valid)\n",
    "    test_losses.append(loss_test)\n",
    "    valid_accuracies.append((pred_valid == y_valid).mean())\n",
    "    test_accuracies.append((pred_test == y_test).mean())\n",
    "    train_losses.append(loss_train)\n",
    "    train_accuracies.append((pred_train == y_train).mean())\n",
    "\n",
    "## Loss figure\n",
    "plt.figure(figsize=(12, 4))\n",
    "axis = plt.subplot(1, 2, 1)\n",
    "axis.plot(range(1, len(train_losses)+1), train_losses, label='train')\n",
    "axis.plot(range(1, len(valid_losses)+1), valid_losses, label='valid')\n",
    "axis.plot(range(1, len(test_losses)+1), test_losses, label='test')\n",
    "axis.legend()\n",
    "axis.set_ylabel('Loss')\n",
    "axis.set_xlabel('Epochs')\n",
    "\n",
    "## Accuracy figure\n",
    "axis = plt.subplot(1, 2, 2)\n",
    "axis.plot(range(1, len(train_accuracies)+1), train_accuracies, label='train')\n",
    "axis.plot(range(1, len(valid_accuracies)+1), valid_accuracies, label='valid')\n",
    "axis.plot(range(1, len(test_accuracies)+1), test_accuracies, label='test')\n",
    "axis.legend()\n",
    "axis.set_ylabel('Accuracy')\n",
    "axis.set_xlabel('Epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "arr = np.asarray([1,2,3])\n",
    "print(arr.shape)\n",
    "print(len(arr.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
